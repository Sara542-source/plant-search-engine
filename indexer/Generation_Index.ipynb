{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76da57c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stock de 615 termes scientifiques/sp√©ciaux charg√©s.\n",
      "--- D√©marrage de l'Indexation Compl√®te (M√©thode Avanc√©e) ---\n",
      "Recherche des fichiers JSON dans '../docs/Plantes'...\n",
      "Recherche des fichiers PDF dans '../docs/Concepts'...\n",
      "\n",
      "D√©but de l'indexation de 95 documents...\n",
      "\n",
      "--- Indexation Termin√©e ---\n",
      "‚úÖ Longueurs des documents (Token Counts) enregistr√©es dans 'document_lengths.json'.\n",
      "‚úÖ Base d'Index finale ('Base_Index.json') enregistr√©e.\n",
      "\n",
      "--- Processus d'Indexation Complet Termin√© ---\n",
      "L'index final ('Base_Index.json') et les longueurs ('document_lengths.json') sont g√©n√©r√©s dans le dossier de l'indexeur.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import pypdf\n",
    "import spacy\n",
    "\n",
    "# --- 1. Param√®tres de Configuration et Pr√©paration spaCy ---\n",
    "\n",
    "# Chemins relatifs √† Plant_search/plant-search-engine/indexer/\n",
    "DOSSIER_PLANTES_JSON = \"../docs/Plantes\"\n",
    "DOSSIER_CONCEPTS_PDF = \"../docs/Concepts\"\n",
    "\n",
    "# FICHIER D'ENTR√âE POUR LE STOCK SCIENTIFIQUE\n",
    "FICHIER_STOCK_SCIENTIFIQUE = \"../docs/mot_scientifique/protected_terms.json\" \n",
    "\n",
    "# Fichiers de sortie (dans le dossier indexer/)\n",
    "FICHIER_INDEX_SORTIE = \"Base_Index.json\"\n",
    "FICHIER_LONGUEURS_SORTIE = \"document_lengths.json\"\n",
    "\n",
    "LANGUE = 'fr_core_news_sm'\n",
    "MAX_NGRAM_SCIENTIFIQUE = 4 # Longueur maximale des N-grammes √† v√©rifier\n",
    "\n",
    "# Chargement du mod√®le spaCy\n",
    "try:\n",
    "    nlp = spacy.load(LANGUE)\n",
    "except OSError:\n",
    "    print(f\"\\nüö® Erreur: Mod√®le spaCy '{LANGUE}' non trouv√©.\")\n",
    "    print(f\"Veuillez l'installer avec : python -m spacy download {LANGUE}\")\n",
    "    exit()\n",
    "\n",
    "# Stock des termes scientifiques charg√© au d√©marrage\n",
    "TERMES_SCIENTIFIQUES_STOCK = set()\n",
    "\n",
    "def charger_stock_scientifique(filepath: str) -> set:\n",
    "    \"\"\"\n",
    "    Charge le stock scientifique depuis un fichier JSON. \n",
    "    Les termes sont convertis en minuscules pour la comparaison.\n",
    "    \"\"\"\n",
    "    stock = set()\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ö†Ô∏è Avertissement: Fichier de stock scientifique '{filepath}' non trouv√©. Le stock est vide.\")\n",
    "        return stock\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            if isinstance(data, list):\n",
    "                stock.update({item.lower().strip() for item in data})\n",
    "            elif isinstance(data, dict):\n",
    "                stock.update({key.lower().strip() for key in data.keys()})\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Avertissement: Format du fichier '{filepath}' non reconnu.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du chargement du stock scientifique: {e}\")\n",
    "        return set()\n",
    "    \n",
    "    print(f\"‚úÖ Stock de {len(stock)} termes scientifiques/sp√©ciaux charg√©s.\")\n",
    "    return stock\n",
    "\n",
    "# Charger le stock d√®s que possible\n",
    "TERMES_SCIENTIFIQUES_STOCK = charger_stock_scientifique(FICHIER_STOCK_SCIENTIFIQUE)\n",
    "\n",
    "\n",
    "# --- 2. Fonctions d'Extraction et de Traitement du Texte ---\n",
    "\n",
    "def extraire_texte_json_avec_termes_separes(filepath: str) -> tuple[str, list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Extrait le texte g√©n√©ral, les termes num√©riques, et les termes Darija.\n",
    "    Les termes Darija sont extraits directement (tels quels, en minuscule).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur d'extraction/lecture JSON √† {filepath}: {e}\")\n",
    "        return \"\", [], []\n",
    "        \n",
    "    texte_general = []\n",
    "    termes_nombres = []\n",
    "    termes_darija_directs = [] # Pour les termes Darija (pris tels quels)\n",
    "    \n",
    "    def parcourir_dict(d):\n",
    "        for key, value in d.items():\n",
    "            if key in ('url', 'urls', 'galerie_images', 'id'):\n",
    "                continue\n",
    "                \n",
    "            # GESTION SP√âCIALE DES NOMS DARIJA (Indexation Directe)\n",
    "            if key == 'noms_darija' and isinstance(value, list):\n",
    "                # Ajout des termes Darija en minuscules, tels quels, sans lemmatisation\n",
    "                termes_darija_directs.extend([t.lower().strip() for t in value if t])\n",
    "                continue # NE PAS AJOUTER AU TEXTE G√âN√âRAL\n",
    "\n",
    "            # Le reste (y compris 'nom_scientifique') est ajout√© au texte g√©n√©ral\n",
    "            if isinstance(value, dict):\n",
    "                parcourir_dict(value)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        parcourir_dict(item)\n",
    "                    elif isinstance(item, str):\n",
    "                        texte_general.append(item)\n",
    "            elif isinstance(value, str):\n",
    "                texte_general.append(value)\n",
    "    \n",
    "    parcourir_dict(data)\n",
    "    \n",
    "    texte_complet = ' '.join(texte_general)\n",
    "    \n",
    "    # 3. Extraction des Nombres (sans leurs unit√©s)\n",
    "    nombres = re.findall(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', texte_complet, re.IGNORECASE)\n",
    "    \n",
    "    for nombre, unite in nombres:\n",
    "        terme_nombre = nombre.replace(',', '.') \n",
    "        termes_nombres.append(terme_nombre)\n",
    "    \n",
    "    # Nettoyer le texte g√©n√©ral des chiffres pour le traitement NLP\n",
    "    texte_nettoye = re.sub(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', ' ', texte_complet, flags=re.IGNORECASE)\n",
    "        \n",
    "    return texte_nettoye, termes_nombres, termes_darija_directs\n",
    "\n",
    "def extraire_texte_pdf(filepath: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    texte = \"\"\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            reader = pypdf.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                texte += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors de l'extraction de texte du PDF {filepath}. Erreur: {e}\")\n",
    "        return \"\"\n",
    "    return texte\n",
    "\n",
    "def identifier_termes_scientifiques(tokens: list, stock_scientifique: set, max_n: int) -> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Identifie et extrait les termes scientifiques/sp√©ciaux multi-mots (N-grammes)\n",
    "    en comparant le texte tokenis√© au stock.\n",
    "    \"\"\"\n",
    "    mots_scientifiques = []\n",
    "    indices_captures = set() \n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i in indices_captures:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        terme_trouve = False\n",
    "        \n",
    "        # Teste les N-grammes de la plus grande taille √† la plus petite (ex: 4, 3, 2, 1)\n",
    "        for n in range(min(max_n, len(tokens) - i), 0, -1):\n",
    "            \n",
    "            ngram_tokens = tokens[i:i+n]\n",
    "            \n",
    "            # Concat√©nation des tokens pour la comparaison\n",
    "            ngram_candidat = \" \".join(ngram_tokens)\n",
    "            ngram_candidat_lower = ngram_candidat.lower().strip()\n",
    "            \n",
    "            if ngram_candidat_lower in stock_scientifique:\n",
    "                # Terme scientifique/sp√©cial trouv√© ! On l'ajoute tel quel.\n",
    "                mots_scientifiques.append(ngram_candidat_lower)\n",
    "                \n",
    "                # Marquer tous les tokens de ce N-gramme comme captur√©s\n",
    "                for j in range(n):\n",
    "                    indices_captures.add(i + j)\n",
    "                \n",
    "                terme_trouve = True\n",
    "                i += n \n",
    "                break\n",
    "        \n",
    "        if not terme_trouve:\n",
    "            i += 1\n",
    "            \n",
    "    # Extraction des tokens restants (mots g√©n√©raux)\n",
    "    tokens_restants = [tokens[i] for i in range(len(tokens)) if i not in indices_captures]\n",
    "    \n",
    "    return mots_scientifiques, tokens_restants\n",
    "\n",
    "\n",
    "def normaliser_texte_lemmatisation_filtree(tokens_restants: list) -> list[str]:\n",
    "    \"\"\"\n",
    "    Lemmatisation stricte des tokens restants (mots g√©n√©raux). \n",
    "    Applique le filtrage : minuscules, enl√®ve ponctuation, stop words, symboles, puis lemmatise.\n",
    "    \"\"\"\n",
    "    texte_restant = \" \".join(tokens_restants) \n",
    "    doc = nlp(texte_restant.lower())\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "    mots_normalises = []\n",
    "    \n",
    "    for token in doc:\n",
    "        lemma_text = token.lemma_.lower().strip()\n",
    "        \n",
    "        # Application du filtrage strict pour les mots g√©n√©raux\n",
    "        if (\n",
    "            not token.is_punct and      # Enl√®ve la ponctuation\n",
    "            token.is_alpha and          # Enl√®ve les symboles et caract√®res non alphab√©tiques\n",
    "            not token.is_stop and       # Enl√®ve les stop words\n",
    "            len(lemma_text) > 1         # Longueur minimale du lemme\n",
    "        ):\n",
    "            mots_normalises.append(lemma_text) \n",
    "            \n",
    "    return mots_normalises\n",
    "\n",
    "# --- 3. Classe Principale d'Indexation ---\n",
    "\n",
    "class IndexeurBotanique:\n",
    "    \n",
    "    def __init__(self, json_dir, pdf_dir):\n",
    "        self.json_dir = json_dir\n",
    "        self.pdf_dir = pdf_dir\n",
    "        # Structure TF simple: { 'mot': { 'document_id': fr√©quence } }\n",
    "        self.index = defaultdict(lambda: defaultdict(int))\n",
    "        self.doc_lengths = {}\n",
    "        \n",
    "    def _creer_doc_id(self, filename: str, doc_type: str) -> str:\n",
    "        \"\"\"Cr√©e l'ID du document en utilisant LE NOM DE FICHIER COMPLET (avec extension).\"\"\"\n",
    "        return filename\n",
    "\n",
    "    def parcourir_documents(self):\n",
    "        \"\"\"Parcourt et indexe TOUS les JSON et TOUS les PDF.\"\"\"\n",
    "        documents_a_indexer = []\n",
    "        \n",
    "        print(f\"Recherche des fichiers JSON dans '{self.json_dir}'...\")\n",
    "        if os.path.isdir(self.json_dir):\n",
    "            for filename in os.listdir(self.json_dir):\n",
    "                if filename.endswith(\".json\"):\n",
    "                    doc_id = self._creer_doc_id(filename, 'json')\n",
    "                    documents_a_indexer.append({'id': doc_id, 'type': 'json', 'path': os.path.join(self.json_dir, filename)})\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur: Dossier JSON '{self.json_dir}' introuvable.\")\n",
    "\n",
    "\n",
    "        print(f\"Recherche des fichiers PDF dans '{self.pdf_dir}'...\")\n",
    "        if os.path.isdir(self.pdf_dir):\n",
    "            for filename in os.listdir(self.pdf_dir):\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    doc_id = self._creer_doc_id(filename, 'pdf')\n",
    "                    documents_a_indexer.append({'id': doc_id, 'type': 'pdf', 'path': os.path.join(self.pdf_dir, filename)})\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur: Dossier PDF '{self.pdf_dir}' introuvable.\")\n",
    "                \n",
    "        print(f\"\\nD√©but de l'indexation de {len(documents_a_indexer)} documents...\")\n",
    "        \n",
    "        for doc_info in documents_a_indexer:\n",
    "             self.indexer_document(doc_info)\n",
    "        print(\"\\n--- Indexation Termin√©e ---\")\n",
    "\n",
    "    # La fonction parcourir_documents_test() est conserv√©e mais non utilis√©e dans le main\n",
    "    def parcourir_documents_test(self, nom_json: str, nom_pdf: str):\n",
    "         \"\"\"Parcourt et indexe seulement 1 JSON et 1 PDF sp√©cifiques pour le test.\"\"\"\n",
    "         # ... (votre impl√©mentation)\n",
    "\n",
    "    def indexer_document(self, doc_info):\n",
    "        \"\"\"Extrait, normalise et construit l'index TF pour un document unique.\"\"\"\n",
    "        doc_id = doc_info['id']\n",
    "        filepath = doc_info['path']\n",
    "        \n",
    "        mots_indexables = []\n",
    "        texte_brut_pour_tokenisation = \"\"\n",
    "        termes_nombres = []\n",
    "        termes_darija_directs = []\n",
    "\n",
    "        # 1. Extraction du texte, nombres et termes Darija\n",
    "        if doc_info['type'] == 'json':\n",
    "            texte_brut_pour_tokenisation, termes_nombres, termes_darija_directs = extraire_texte_json_avec_termes_separes(filepath)\n",
    "            \n",
    "        elif doc_info['type'] == 'pdf':\n",
    "            texte_complet_pdf = extraire_texte_pdf(filepath)\n",
    "            if not texte_complet_pdf: return \n",
    "            \n",
    "            # Extraction des Nombres (PDF)\n",
    "            nombres = re.findall(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', texte_complet_pdf, re.IGNORECASE)\n",
    "            for nombre, unite in nombres:\n",
    "                 termes_nombres.append(nombre.replace(',', '.')) \n",
    "            \n",
    "            # Nettoyage du texte g√©n√©ral des chiffres pour le traitement NLP\n",
    "            texte_brut_pour_tokenisation = re.sub(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', ' ', texte_complet_pdf, flags=re.IGNORECASE)\n",
    "\n",
    "        # 2. AJOUT DIRECT des Termes (Nombres et Darija)\n",
    "        mots_indexables.extend(termes_nombres)\n",
    "        mots_indexables.extend(termes_darija_directs) \n",
    "        \n",
    "        # 3. Tokenisation pour la reconnaissance des N-grammes\n",
    "        doc = nlp(texte_brut_pour_tokenisation)\n",
    "        tokens_bruts = [token.text for token in doc if not token.is_space]\n",
    "        \n",
    "        # 4. Reconnaissance des Termes Scientifiques (non lemmatis√©s)\n",
    "        mots_scientifiques, tokens_restants = identifier_termes_scientifiques(\n",
    "            tokens_bruts, TERMES_SCIENTIFIQUES_STOCK, MAX_NGRAM_SCIENTIFIQUE\n",
    "        )\n",
    "        mots_indexables.extend(mots_scientifiques) \n",
    "        \n",
    "        # 5. Lemmatisation des Mots G√©n√©raux Restants\n",
    "        mots_lemmatises = normaliser_texte_lemmatisation_filtree(tokens_restants)\n",
    "        mots_indexables.extend(mots_lemmatises)\n",
    "\n",
    "        if not mots_indexables: return\n",
    "\n",
    "        # 6. Enregistrement de la longueur et construction de l'index\n",
    "        self.doc_lengths[doc_id] = len(mots_indexables)\n",
    "        \n",
    "        for mot in mots_indexables:\n",
    "            if mot:\n",
    "                self.index[mot][doc_id] += 1 \n",
    "\n",
    "    def enregistrer_longueurs(self, output_file):\n",
    "        \"\"\"Enregistre les longueurs des documents.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.doc_lengths, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Longueurs des documents (Token Counts) enregistr√©es dans '{output_file}'.\")\n",
    "\n",
    "    def enregistrer_index(self, output_file):\n",
    "        \"\"\"Enregistre l'index invers√© final (Base d'Index) au format simplifi√©.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)\n",
    "        \n",
    "        index_a_sauvegarder = dict(self.index)\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(index_a_sauvegarder, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"‚úÖ Base d'Index finale ('{output_file}') enregistr√©e.\")\n",
    "\n",
    "\n",
    "# --- 4. Ex√©cution du Script Principal (Mode Production) ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- D√©marrage de l'Indexation Compl√®te (M√©thode Avanc√©e) ---\")\n",
    "\n",
    "    # Le stock scientifique est d√©j√† charg√© au d√©but du script\n",
    "\n",
    "    # 1. Initialisation de l'Indexeur\n",
    "    indexeur = IndexeurBotanique(\n",
    "        json_dir=DOSSIER_PLANTES_JSON,\n",
    "        pdf_dir=DOSSIER_CONCEPTS_PDF\n",
    "    )\n",
    "    \n",
    "    # 2. Ex√©cution de l'indexation sur TOUS les fichiers\n",
    "    indexeur.parcourir_documents()\n",
    "    \n",
    "    # 3. Enregistrement des Fichiers de Sortie\n",
    "    indexeur.enregistrer_longueurs(FICHIER_LONGUEURS_SORTIE)\n",
    "    indexeur.enregistrer_index(FICHIER_INDEX_SORTIE)\n",
    "\n",
    "    print(\"\\n--- Processus d'Indexation Complet Termin√© ---\")\n",
    "    print(f\"L'index final ('{FICHIER_INDEX_SORTIE}') et les longueurs ('{FICHIER_LONGUEURS_SORTIE}') sont g√©n√©r√©s dans le dossier de l'indexeur.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
