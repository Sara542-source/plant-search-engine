{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f5a7a4",
   "metadata": {},
   "source": [
    "Processus d'Indexation (Base Index)\n",
    "Le script Generation_Index.ipynb parcourt l'ensemble des donn√©es (JSON et PDF) pour construire une base d'index invers√© (Base_Index.json) et un fichier de longueurs de documents (document_lengths.json), essentiels pour le calcul TF-IDF.\n",
    "\n",
    "√âtapes Cl√©s :\n",
    "Chargement du Stock Prot√©g√© :\n",
    "\n",
    "Le fichier protected_terms.json est charg√©. Ces termes (noms scientifiques, Darija, etc.) sont mis en minuscules et utilis√©s pour prot√©ger des termes sp√©cifiques de la lemmatisation.\n",
    "\n",
    "Parcours et Extraction des Documents :\n",
    "\n",
    "Le code it√®re sur tous les fichiers JSON (../docs/Plantes) et PDF (../docs/Concepts).\n",
    "\n",
    "Les ID des documents sont bas√©s sur le nom de fichier.\n",
    "\n",
    "Traitement des Termes Sp√©ciaux (Indexation Directe) :\n",
    "\n",
    "Nombres : Les valeurs num√©riques (ex: 30, 1.5) sont extraites du texte et ajout√©es aux tokens sans modification.\n",
    "\n",
    "Darija : Les termes du champ noms_darija sont extraits et ajout√©s tels quels, en minuscules.\n",
    "\n",
    "Reconnaissance des Termes Scientifiques (N-grams) :\n",
    "\n",
    "Le texte g√©n√©ral est tokenis√©.\n",
    "\n",
    "Les s√©quences de tokens (jusqu'√† 4 mots, ou N-grams) sont compar√©es au Stock Prot√©g√©.\n",
    "\n",
    "Si une s√©quence correspond (ex: \"petroselinum crispum\"), elle est captur√©e comme un terme unique et ajout√©e aux tokens indexables. Les tokens composant ce terme sont exclus de la lemmatisation.\n",
    "\n",
    "Normalisation et Lemmatisation (Mots G√©n√©raux) :\n",
    "\n",
    "Les tokens restants (mots g√©n√©raux) sont trait√©s par spaCy.\n",
    "\n",
    "Ils sont filtr√©s (suppression des stop words, ponctuation, symboles).\n",
    "\n",
    "Ils sont r√©duits √† leur lemme (ex: \"cultiv√©es\" ‚Üí \"cultiv√©\").\n",
    "\n",
    "Construction de l'Index Invers√© (TF) :\n",
    "\n",
    "L'index est rempli en comptant la fr√©quence brute (TF) de chaque occurrence de chaque token (lemme ou terme sp√©cial) dans chaque document.\n",
    "\n",
    "Calcul des Longueurs :\n",
    "\n",
    "La longueur du document est d√©finie comme le nombre total de tokens indexables (y compris les r√©p√©titions) et est enregistr√©e dans document_lengths.json.\n",
    "\n",
    "Sauvegarde :\n",
    "\n",
    "Les fichiers Base_Index.json et document_lengths.json sont sauvegard√©s dans le dossier de l'indexeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76da57c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stock de 615 termes scientifiques/sp√©ciaux charg√©s.\n",
      "--- D√©marrage de l'Indexation Compl√®te (M√©thode Avanc√©e) ---\n",
      "Recherche des fichiers JSON dans '../docs/Plantes'...\n",
      "Recherche des fichiers PDF dans '../docs/Concepts'...\n",
      "\n",
      "D√©but de l'indexation de 95 documents...\n",
      "\n",
      "--- Indexation Termin√©e ---\n",
      "‚úÖ Longueurs des documents (Token Counts) enregistr√©es dans 'document_lengths.json'.\n",
      "‚úÖ Base d'Index finale ('Base_Index.json') enregistr√©e.\n",
      "\n",
      "--- Processus d'Indexation Complet Termin√© ---\n",
      "L'index final ('Base_Index.json') et les longueurs ('document_lengths.json') sont g√©n√©r√©s dans le dossier de l'indexeur.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import pypdf\n",
    "import spacy\n",
    "\n",
    "# --- 1. Param√®tres de Configuration et Pr√©paration spaCy ---\n",
    "\n",
    "# Chemins relatifs √† Plant_search/plant-search-engine/indexer/\n",
    "DOSSIER_PLANTES_JSON = \"../docs/Plantes\"\n",
    "DOSSIER_CONCEPTS_PDF = \"../docs/Concepts\"\n",
    "\n",
    "# FICHIER D'ENTR√âE POUR LE STOCK SCIENTIFIQUE\n",
    "FICHIER_STOCK_SCIENTIFIQUE = \"../docs/mot_scientifique/protected_terms.json\" \n",
    "\n",
    "# Fichiers de sortie (dans le dossier indexer/)\n",
    "FICHIER_INDEX_SORTIE = \"Base_Index.json\"\n",
    "FICHIER_LONGUEURS_SORTIE = \"document_lengths.json\"\n",
    "\n",
    "LANGUE = 'fr_core_news_sm'\n",
    "MAX_NGRAM_SCIENTIFIQUE = 4 # Longueur maximale des N-grammes √† v√©rifier\n",
    "\n",
    "# Chargement du mod√®le spaCy\n",
    "try:\n",
    "    nlp = spacy.load(LANGUE)\n",
    "except OSError:\n",
    "    print(f\"\\nüö® Erreur: Mod√®le spaCy '{LANGUE}' non trouv√©.\")\n",
    "    print(f\"Veuillez l'installer avec : python -m spacy download {LANGUE}\")\n",
    "    exit()\n",
    "\n",
    "# Stock des termes scientifiques charg√© au d√©marrage\n",
    "TERMES_SCIENTIFIQUES_STOCK = set()\n",
    "\n",
    "def charger_stock_scientifique(filepath: str) -> set:\n",
    "    \"\"\"\n",
    "    Charge le stock scientifique depuis un fichier JSON. \n",
    "    Les termes sont convertis en minuscules pour la comparaison.\n",
    "    \"\"\"\n",
    "    stock = set()\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ö†Ô∏è Avertissement: Fichier de stock scientifique '{filepath}' non trouv√©. Le stock est vide.\")\n",
    "        return stock\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            if isinstance(data, list):\n",
    "                stock.update({item.lower().strip() for item in data})\n",
    "            elif isinstance(data, dict):\n",
    "                stock.update({key.lower().strip() for key in data.keys()})\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Avertissement: Format du fichier '{filepath}' non reconnu.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du chargement du stock scientifique: {e}\")\n",
    "        return set()\n",
    "    \n",
    "    print(f\"‚úÖ Stock de {len(stock)} termes scientifiques/sp√©ciaux charg√©s.\")\n",
    "    return stock\n",
    "\n",
    "# Charger le stock d√®s que possible\n",
    "TERMES_SCIENTIFIQUES_STOCK = charger_stock_scientifique(FICHIER_STOCK_SCIENTIFIQUE)\n",
    "\n",
    "\n",
    "# --- 2. Fonctions d'Extraction et de Traitement du Texte ---\n",
    "\n",
    "def extraire_texte_json_avec_termes_separes(filepath: str) -> tuple[str, list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Extrait le texte g√©n√©ral, les termes num√©riques, et les termes Darija.\n",
    "    Les termes Darija sont extraits directement (tels quels, en minuscule).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur d'extraction/lecture JSON √† {filepath}: {e}\")\n",
    "        return \"\", [], []\n",
    "        \n",
    "    texte_general = []\n",
    "    termes_nombres = []\n",
    "    termes_darija_directs = [] # Pour les termes Darija (pris tels quels)\n",
    "    \n",
    "    def parcourir_dict(d):\n",
    "        for key, value in d.items():\n",
    "            if key in ('url', 'urls', 'galerie_images', 'id'):\n",
    "                continue\n",
    "                \n",
    "            # GESTION SP√âCIALE DES NOMS DARIJA (Indexation Directe)\n",
    "            if key == 'noms_darija' and isinstance(value, list):\n",
    "                # Ajout des termes Darija en minuscules, tels quels, sans lemmatisation\n",
    "                termes_darija_directs.extend([t.lower().strip() for t in value if t])\n",
    "                continue # NE PAS AJOUTER AU TEXTE G√âN√âRAL\n",
    "\n",
    "            # Le reste (y compris 'nom_scientifique') est ajout√© au texte g√©n√©ral\n",
    "            if isinstance(value, dict):\n",
    "                parcourir_dict(value)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        parcourir_dict(item)\n",
    "                    elif isinstance(item, str):\n",
    "                        texte_general.append(item)\n",
    "            elif isinstance(value, str):\n",
    "                texte_general.append(value)\n",
    "    \n",
    "    parcourir_dict(data)\n",
    "    \n",
    "    texte_complet = ' '.join(texte_general)\n",
    "    \n",
    "    # 3. Extraction des Nombres (sans leurs unit√©s)\n",
    "    nombres = re.findall(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', texte_complet, re.IGNORECASE)\n",
    "    \n",
    "    for nombre, unite in nombres:\n",
    "        terme_nombre = nombre.replace(',', '.') \n",
    "        termes_nombres.append(terme_nombre)\n",
    "    \n",
    "    # Nettoyer le texte g√©n√©ral des chiffres pour le traitement NLP\n",
    "    texte_nettoye = re.sub(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', ' ', texte_complet, flags=re.IGNORECASE)\n",
    "        \n",
    "    return texte_nettoye, termes_nombres, termes_darija_directs\n",
    "\n",
    "def extraire_texte_pdf(filepath: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    texte = \"\"\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            reader = pypdf.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                texte += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors de l'extraction de texte du PDF {filepath}. Erreur: {e}\")\n",
    "        return \"\"\n",
    "    return texte\n",
    "\n",
    "def identifier_termes_scientifiques(tokens: list, stock_scientifique: set, max_n: int) -> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Identifie et extrait les termes scientifiques/sp√©ciaux multi-mots (N-grammes)\n",
    "    en comparant le texte tokenis√© au stock.\n",
    "    \"\"\"\n",
    "    mots_scientifiques = []\n",
    "    indices_captures = set() \n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i in indices_captures:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        terme_trouve = False\n",
    "        \n",
    "        # Teste les N-grammes de la plus grande taille √† la plus petite (ex: 4, 3, 2, 1)\n",
    "        for n in range(min(max_n, len(tokens) - i), 0, -1):\n",
    "            \n",
    "            ngram_tokens = tokens[i:i+n]\n",
    "            \n",
    "            # Concat√©nation des tokens pour la comparaison\n",
    "            ngram_candidat = \" \".join(ngram_tokens)\n",
    "            ngram_candidat_lower = ngram_candidat.lower().strip()\n",
    "            \n",
    "            if ngram_candidat_lower in stock_scientifique:\n",
    "                # Terme scientifique/sp√©cial trouv√© ! On l'ajoute tel quel.\n",
    "                mots_scientifiques.append(ngram_candidat_lower)\n",
    "                \n",
    "                # Marquer tous les tokens de ce N-gramme comme captur√©s\n",
    "                for j in range(n):\n",
    "                    indices_captures.add(i + j)\n",
    "                \n",
    "                terme_trouve = True\n",
    "                i += n \n",
    "                break\n",
    "        \n",
    "        if not terme_trouve:\n",
    "            i += 1\n",
    "            \n",
    "    # Extraction des tokens restants (mots g√©n√©raux)\n",
    "    tokens_restants = [tokens[i] for i in range(len(tokens)) if i not in indices_captures]\n",
    "    \n",
    "    return mots_scientifiques, tokens_restants\n",
    "\n",
    "\n",
    "def normaliser_texte_lemmatisation_filtree(tokens_restants: list) -> list[str]:\n",
    "    \"\"\"\n",
    "    Lemmatisation stricte des tokens restants (mots g√©n√©raux). \n",
    "    Applique le filtrage : minuscules, enl√®ve ponctuation, stop words, symboles, puis lemmatise.\n",
    "    \"\"\"\n",
    "    texte_restant = \" \".join(tokens_restants) \n",
    "    doc = nlp(texte_restant.lower())\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "    mots_normalises = []\n",
    "    \n",
    "    for token in doc:\n",
    "        lemma_text = token.lemma_.lower().strip()\n",
    "        \n",
    "        # Application du filtrage strict pour les mots g√©n√©raux\n",
    "        if (\n",
    "            not token.is_punct and      # Enl√®ve la ponctuation\n",
    "            token.is_alpha and          # Enl√®ve les symboles et caract√®res non alphab√©tiques\n",
    "            not token.is_stop and       # Enl√®ve les stop words\n",
    "            len(lemma_text) > 1         # Longueur minimale du lemme\n",
    "        ):\n",
    "            mots_normalises.append(lemma_text) \n",
    "            \n",
    "    return mots_normalises\n",
    "\n",
    "# --- 3. Classe Principale d'Indexation ---\n",
    "\n",
    "class IndexeurBotanique:\n",
    "    \n",
    "    def __init__(self, json_dir, pdf_dir):\n",
    "        self.json_dir = json_dir\n",
    "        self.pdf_dir = pdf_dir\n",
    "        # Structure TF simple: { 'mot': { 'document_id': fr√©quence } }\n",
    "        self.index = defaultdict(lambda: defaultdict(int))\n",
    "        self.doc_lengths = {}\n",
    "        \n",
    "    def _creer_doc_id(self, filename: str, doc_type: str) -> str:\n",
    "        \"\"\"Cr√©e l'ID du document en utilisant LE NOM DE FICHIER COMPLET (avec extension).\"\"\"\n",
    "        return filename\n",
    "\n",
    "    def parcourir_documents(self):\n",
    "        \"\"\"Parcourt et indexe TOUS les JSON et TOUS les PDF.\"\"\"\n",
    "        documents_a_indexer = []\n",
    "        \n",
    "        print(f\"Recherche des fichiers JSON dans '{self.json_dir}'...\")\n",
    "        if os.path.isdir(self.json_dir):\n",
    "            for filename in os.listdir(self.json_dir):\n",
    "                if filename.endswith(\".json\"):\n",
    "                    doc_id = self._creer_doc_id(filename, 'json')\n",
    "                    documents_a_indexer.append({'id': doc_id, 'type': 'json', 'path': os.path.join(self.json_dir, filename)})\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur: Dossier JSON '{self.json_dir}' introuvable.\")\n",
    "\n",
    "\n",
    "        print(f\"Recherche des fichiers PDF dans '{self.pdf_dir}'...\")\n",
    "        if os.path.isdir(self.pdf_dir):\n",
    "            for filename in os.listdir(self.pdf_dir):\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    doc_id = self._creer_doc_id(filename, 'pdf')\n",
    "                    documents_a_indexer.append({'id': doc_id, 'type': 'pdf', 'path': os.path.join(self.pdf_dir, filename)})\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur: Dossier PDF '{self.pdf_dir}' introuvable.\")\n",
    "                \n",
    "        print(f\"\\nD√©but de l'indexation de {len(documents_a_indexer)} documents...\")\n",
    "        \n",
    "        for doc_info in documents_a_indexer:\n",
    "             self.indexer_document(doc_info)\n",
    "        print(\"\\n--- Indexation Termin√©e ---\")\n",
    "\n",
    "    # La fonction parcourir_documents_test() est conserv√©e mais non utilis√©e dans le main\n",
    "    def parcourir_documents_test(self, nom_json: str, nom_pdf: str):\n",
    "         \"\"\"Parcourt et indexe seulement 1 JSON et 1 PDF sp√©cifiques pour le test.\"\"\"\n",
    "         # ... (votre impl√©mentation)\n",
    "\n",
    "    def indexer_document(self, doc_info):\n",
    "        \"\"\"Extrait, normalise et construit l'index TF pour un document unique.\"\"\"\n",
    "        doc_id = doc_info['id']\n",
    "        filepath = doc_info['path']\n",
    "        \n",
    "        mots_indexables = []\n",
    "        texte_brut_pour_tokenisation = \"\"\n",
    "        termes_nombres = []\n",
    "        termes_darija_directs = []\n",
    "\n",
    "        # 1. Extraction du texte, nombres et termes Darija\n",
    "        if doc_info['type'] == 'json':\n",
    "            texte_brut_pour_tokenisation, termes_nombres, termes_darija_directs = extraire_texte_json_avec_termes_separes(filepath)\n",
    "            \n",
    "        elif doc_info['type'] == 'pdf':\n",
    "            texte_complet_pdf = extraire_texte_pdf(filepath)\n",
    "            if not texte_complet_pdf: return \n",
    "            \n",
    "            # Extraction des Nombres (PDF)\n",
    "            nombres = re.findall(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', texte_complet_pdf, re.IGNORECASE)\n",
    "            for nombre, unite in nombres:\n",
    "                 termes_nombres.append(nombre.replace(',', '.')) \n",
    "            \n",
    "            # Nettoyage du texte g√©n√©ral des chiffres pour le traitement NLP\n",
    "            texte_brut_pour_tokenisation = re.sub(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', ' ', texte_complet_pdf, flags=re.IGNORECASE)\n",
    "\n",
    "        # 2. AJOUT DIRECT des Termes (Nombres et Darija)\n",
    "        mots_indexables.extend(termes_nombres)\n",
    "        mots_indexables.extend(termes_darija_directs) \n",
    "        \n",
    "        # 3. Tokenisation pour la reconnaissance des N-grammes\n",
    "        doc = nlp(texte_brut_pour_tokenisation)\n",
    "        tokens_bruts = [token.text for token in doc if not token.is_space]\n",
    "        \n",
    "        # 4. Reconnaissance des Termes Scientifiques (non lemmatis√©s)\n",
    "        mots_scientifiques, tokens_restants = identifier_termes_scientifiques(\n",
    "            tokens_bruts, TERMES_SCIENTIFIQUES_STOCK, MAX_NGRAM_SCIENTIFIQUE\n",
    "        )\n",
    "        mots_indexables.extend(mots_scientifiques) \n",
    "        \n",
    "        # 5. Lemmatisation des Mots G√©n√©raux Restants\n",
    "        mots_lemmatises = normaliser_texte_lemmatisation_filtree(tokens_restants)\n",
    "        mots_indexables.extend(mots_lemmatises)\n",
    "\n",
    "        if not mots_indexables: return\n",
    "\n",
    "        # 6. Enregistrement de la longueur et construction de l'index\n",
    "        self.doc_lengths[doc_id] = len(mots_indexables)\n",
    "        \n",
    "        for mot in mots_indexables:\n",
    "            if mot:\n",
    "                self.index[mot][doc_id] += 1 \n",
    "\n",
    "    def enregistrer_longueurs(self, output_file):\n",
    "        \"\"\"Enregistre les longueurs des documents.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.doc_lengths, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Longueurs des documents (Token Counts) enregistr√©es dans '{output_file}'.\")\n",
    "\n",
    "    def enregistrer_index(self, output_file):\n",
    "        \"\"\"Enregistre l'index invers√© final (Base d'Index) au format simplifi√©.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)\n",
    "        \n",
    "        index_a_sauvegarder = dict(self.index)\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(index_a_sauvegarder, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"‚úÖ Base d'Index finale ('{output_file}') enregistr√©e.\")\n",
    "\n",
    "\n",
    "# --- 4. Ex√©cution du Script Principal (Mode Production) ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- D√©marrage de l'Indexation Compl√®te (M√©thode Avanc√©e) ---\")\n",
    "\n",
    "    # Le stock scientifique est d√©j√† charg√© au d√©but du script\n",
    "\n",
    "    # 1. Initialisation de l'Indexeur\n",
    "    indexeur = IndexeurBotanique(\n",
    "        json_dir=DOSSIER_PLANTES_JSON,\n",
    "        pdf_dir=DOSSIER_CONCEPTS_PDF\n",
    "    )\n",
    "    \n",
    "    # 2. Ex√©cution de l'indexation sur TOUS les fichiers\n",
    "    indexeur.parcourir_documents()\n",
    "    \n",
    "    # 3. Enregistrement des Fichiers de Sortie\n",
    "    indexeur.enregistrer_longueurs(FICHIER_LONGUEURS_SORTIE)\n",
    "    indexeur.enregistrer_index(FICHIER_INDEX_SORTIE)\n",
    "\n",
    "    print(\"\\n--- Processus d'Indexation Complet Termin√© ---\")\n",
    "    print(f\"L'index final ('{FICHIER_INDEX_SORTIE}') et les longueurs ('{FICHIER_LONGUEURS_SORTIE}') sont g√©n√©r√©s dans le dossier de l'indexeur.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf3f09",
   "metadata": {},
   "source": [
    "Ensuite genration des vocabulaires qui contient que les tokens pour les json et pour les pdf  separ√©s.\n",
    "pour un usage ulterieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "038bd27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- D√©marrage de la Reconstruction du Vocabulaire depuis l'Index ---\n",
      "‚úÖ Base d'Index charg√©e. Contient 6748 tokens uniques.\n",
      "\n",
      "--- R√©sultat du Tri ---\n",
      "Total des tokens uniques dans l'Index : 6748\n",
      "Tokens class√©s comme 'Plantes' (JSON) : 4673\n",
      "Tokens class√©s comme 'Concepts' (PDF) : 3592\n",
      "‚úÖ Vocabulaire Plantes (JSON) enregistr√© dans 'token_plante.json' (4673 mots).\n",
      "‚úÖ Vocabulaire Concepts (PDF) enregistr√© dans 'token_concept.json' (3592 mots).\n",
      "\n",
      "--- Reconstruction du Vocabulaire bas√©e sur l'Index Termin√©e ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Set, Dict, List, Tuple\n",
    "\n",
    "# --- 1. Chemins des Fichiers ---\n",
    "\n",
    "FICHIER_INDEX_SORTIE = \"Base_Index.json\"\n",
    "# Nouveaux fichiers de sortie\n",
    "FICHIER_TOKEN_PLANTE = \"token_plante.json\"\n",
    "FICHIER_TOKEN_CONCEPT = \"token_concept.json\"\n",
    "\n",
    "# D√©finition des pr√©fixes ou extensions pour identifier les types de documents\n",
    "PREFIXE_PLANTE = \".json\"  # ID de documents JSON\n",
    "PREFIXE_CONCEPT = \".pdf\" # ID de documents PDF\n",
    "\n",
    "# --- 2. Fonction de Chargement ---\n",
    "\n",
    "def charger_base_index(filepath: str) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"Charge la Base d'Index Invers√©.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ùå ERREUR : Fichier Base d'Index non trouv√© √† {filepath}. Annulation.\")\n",
    "        return {}\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, dict):\n",
    "                print(f\"‚úÖ Base d'Index charg√©e. Contient {len(data)} tokens uniques.\")\n",
    "                # La structure attendue est Dict[token, Dict[doc_id, freq]]\n",
    "                return data\n",
    "            else:\n",
    "                print(\"‚ùå ERREUR : Le fichier Base d'Index n'est pas un dictionnaire.\")\n",
    "                return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ùå ERREUR JSON : Le fichier Base d'Index n'est pas un JSON valide.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR de lecture : {e}\")\n",
    "        return {}\n",
    "\n",
    "# --- 3. Fonction d'Extraction et de Tri (CORRIG√âE) ---\n",
    "\n",
    "def extraire_et_trier_tokens(base_index: Dict[str, Dict[str, int]]) -> Tuple[Set[str], Set[str]]:\n",
    "    \"\"\"\n",
    "    Parcourt l'index (cl√©=token, valeur=Dict[doc_id, freq]) et trie chaque token \n",
    "    dans les ensembles Plantes et Concepts.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens_plantes: Set[str] = set()\n",
    "    tokens_concepts: Set[str] = set()\n",
    "    \n",
    "    total_tokens_parcourus = 0\n",
    "    \n",
    "    # It√©ration sur les tokens (Cl√© = le mot/token)\n",
    "    for token, postings in base_index.items():\n",
    "        total_tokens_parcourus += 1\n",
    "        \n",
    "        token_nettoye = token.lower().strip()\n",
    "        if not token_nettoye:\n",
    "            continue\n",
    "            \n",
    "        est_plante = False\n",
    "        est_concept = False\n",
    "        \n",
    "        # It√©ration sur le dictionnaire de postings (Cl√© = doc_id, Valeur = fr√©quence)\n",
    "        for doc_id, frequence in postings.items(): \n",
    "            \n",
    "            # Classification bas√©e sur la fin du doc_id\n",
    "            if doc_id.endswith(PREFIXE_PLANTE):\n",
    "                est_plante = True\n",
    "            elif doc_id.endswith(PREFIXE_CONCEPT):\n",
    "                est_concept = True\n",
    "            \n",
    "            # Si nous avons trouv√© les deux cat√©gories, nous pouvons arr√™ter cette boucle interne\n",
    "            if est_plante and est_concept:\n",
    "                break \n",
    "                \n",
    "        # Stockage du token dans les ensembles finaux\n",
    "        if est_plante:\n",
    "            tokens_plantes.add(token_nettoye)\n",
    "        \n",
    "        if est_concept:\n",
    "            tokens_concepts.add(token_nettoye)\n",
    "            \n",
    "    print(f\"\\n--- R√©sultat du Tri ---\")\n",
    "    print(f\"Total des tokens uniques dans l'Index : {total_tokens_parcourus}\")\n",
    "    print(f\"Tokens class√©s comme 'Plantes' (JSON) : {len(tokens_plantes)}\")\n",
    "    print(f\"Tokens class√©s comme 'Concepts' (PDF) : {len(tokens_concepts)}\")\n",
    "    \n",
    "    return tokens_plantes, tokens_concepts\n",
    "\n",
    "# --- 4. Fonction de Sauvegarde ---\n",
    "\n",
    "def sauvegarder_vocabulaire(vocabulaire: Set[str], filepath: str, nom_type: str):\n",
    "    \"\"\"Sauvegarde l'ensemble de tokens tri√© dans un fichier JSON.\"\"\"\n",
    "    try:\n",
    "        liste_triee = sorted(list(vocabulaire))\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(liste_triee, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"‚úÖ Vocabulaire {nom_type} enregistr√© dans '{filepath}' ({len(liste_triee)} mots).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR lors de la sauvegarde du fichier {filepath} : {e}\")\n",
    "\n",
    "# --- 5. Ex√©cution du Script Principal ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- D√©marrage de la Reconstruction du Vocabulaire depuis l'Index ---\")\n",
    "\n",
    "    # 1. Chargement de la Base d'Index\n",
    "    base_index = charger_base_index(FICHIER_INDEX_SORTIE)\n",
    "    \n",
    "    if not base_index:\n",
    "        exit()\n",
    "\n",
    "    # 2. Extraction et Tri des Tokens\n",
    "    tokens_plantes, tokens_concepts = extraire_et_trier_tokens(base_index)\n",
    "    \n",
    "    # 3. Sauvegarde des Nouveaux Fichiers de Vocabulaire\n",
    "    sauvegarder_vocabulaire(tokens_plantes, FICHIER_TOKEN_PLANTE, \"Plantes (JSON)\")\n",
    "    sauvegarder_vocabulaire(tokens_concepts, FICHIER_TOKEN_CONCEPT, \"Concepts (PDF)\")\n",
    "\n",
    "    print(\"\\n--- Reconstruction du Vocabulaire bas√©e sur l'Index Termin√©e ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391a66c",
   "metadata": {},
   "source": [
    "teste et extraction des termes scientifiques dans les concepts en pdf,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0db41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chargement r√©ussi de 615 termes scientifiques/prot√©g√©s.\n",
      "‚úÖ Chargement r√©ussi de 3592 tokens de concepts.\n",
      "üîç D√©tection termin√©e. 78 concepts scientifiques trouv√©s.\n",
      "üíæ Succ√®s: Les concepts scientifiques ont √©t√© enregistr√©s dans science_concept.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# --- D√©finition des chemins de fichiers (Ajustez si n√©cessaire) ---\n",
    "# NOTE : J'utilise ici les noms de fichiers que vous avez fournis.\n",
    "TOKEN_CONCEPTS_FILE = '../doc/Token/token_concept.json'\n",
    "PROTECTED_TERMS_FILE = '../docs/mot_scientifique/protected_terms.json'\n",
    "OUTPUT_FILE = '../docs/Token/science_concept.json'\n",
    "\n",
    "\n",
    "def detect_scientific_concepts(tokens_path, protected_terms_path, output_path):\n",
    "    \"\"\"\n",
    "    D√©tecte les mots scientifiques/techniques dans la liste des tokens \n",
    "    en les comparant √† une liste de termes prot√©g√©s et g√©n√®re un fichier JSON.\n",
    "    \"\"\"\n",
    "    # 1. Chargement des termes prot√©g√©s (Mots Scientifiques)\n",
    "    try:\n",
    "        with open(protected_terms_path, 'r', encoding='utf-8') as f:\n",
    "            # protected_terms.json semble √™tre une simple liste de cha√Ænes\n",
    "            protected_terms_list = json.load(f)\n",
    "            # Convertir la liste en un ensemble (set) pour une recherche O(1) rapide\n",
    "            protected_terms_set = {term.lower() for term in protected_terms_list}\n",
    "        print(f\"‚úÖ Chargement r√©ussi de {len(protected_terms_set)} termes scientifiques/prot√©g√©s.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Erreur: Le fichier des termes prot√©g√©s n'a pas √©t√© trouv√© √† {protected_terms_path}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"‚ùå Erreur de d√©codage JSON dans le fichier {protected_terms_path}\")\n",
    "        return\n",
    "\n",
    "    # 2. Chargement des tokens de concepts\n",
    "    try:\n",
    "        with open(tokens_path, 'r', encoding='utf-8') as f:\n",
    "            # token_concept.json semble √™tre une simple liste de cha√Ænes\n",
    "            concept_tokens_list = json.load(f)\n",
    "        print(f\"‚úÖ Chargement r√©ussi de {len(concept_tokens_list)} tokens de concepts.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Erreur: Le fichier des tokens n'a pas √©t√© trouv√© √† {tokens_path}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"‚ùå Erreur de d√©codage JSON dans le fichier {tokens_path}\")\n",
    "        return\n",
    "\n",
    "    # 3. D√©tection des concepts scientifiques\n",
    "    scientific_concepts = set()\n",
    "    \n",
    "    # Parcourir chaque token et v√©rifier s'il est dans l'ensemble des termes prot√©g√©s.\n",
    "    # On met tout en minuscules pour assurer une correspondance insensible √† la casse.\n",
    "    for token in concept_tokens_list:\n",
    "        token_lower = token.lower()\n",
    "        if token_lower in protected_terms_set:\n",
    "            scientific_concepts.add(token) # Conserver la casse originale du token\n",
    "\n",
    "    scientific_concepts_list = sorted(list(scientific_concepts))\n",
    "    print(f\"üîç D√©tection termin√©e. {len(scientific_concepts_list)} concepts scientifiques trouv√©s.\")\n",
    "\n",
    "    # 4. Enregistrement du r√©sultat\n",
    "    try:\n",
    "        # Assurez-vous que le r√©pertoire de sortie existe\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir: # Ajout d'une v√©rification pour s'assurer que le chemin n'est pas vide\n",
    "             os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(scientific_concepts_list, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üíæ Succ√®s: Les concepts scientifiques ont √©t√© enregistr√©s dans {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l'enregistrement du fichier {output_path}: {e}\")\n",
    "\n",
    "# --- Ex√©cution du script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Ligne supprim√©e ou remplac√©e : ---\n",
    "    # L'ancien code causait l'erreur: os.makedirs(os.path.dirname(TOKEN_CONCEPTS_FILE), exist_ok=True)\n",
    "    # Remplac√© par une simple v√©rification de chemin si n√©cessaire :\n",
    "    if os.path.dirname(TOKEN_CONCEPTS_FILE):\n",
    "        os.makedirs(os.path.dirname(TOKEN_CONCEPTS_FILE), exist_ok=True)\n",
    "    \n",
    "    # Le script suppose que vous avez cr√©√© les fichiers d'entr√©e avant de l'ex√©cuter.\n",
    "    # Ex√©cutez la fonction de d√©tection\n",
    "    detect_scientific_concepts(TOKEN_CONCEPTS_FILE, PROTECTED_TERMS_FILE, OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
