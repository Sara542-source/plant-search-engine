{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76da57c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stock de 615 termes scientifiques/sp√©ciaux charg√©s.\n",
      "--- D√©marrage de l'Indexation Compl√®te (M√©thode Avanc√©e) ---\n",
      "Recherche des fichiers JSON dans '../docs/Plantes'...\n",
      "Recherche des fichiers PDF dans '../docs/Concepts'...\n",
      "\n",
      "D√©but de l'indexation de 95 documents...\n",
      "\n",
      "--- Indexation Termin√©e ---\n",
      "‚úÖ Longueurs des documents (Token Counts) enregistr√©es dans 'document_lengths.json'.\n",
      "‚úÖ Base d'Index finale ('Base_Index.json') enregistr√©e.\n",
      "\n",
      "--- Processus d'Indexation Complet Termin√© ---\n",
      "L'index final ('Base_Index.json') et les longueurs ('document_lengths.json') sont g√©n√©r√©s dans le dossier de l'indexeur.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import pypdf\n",
    "import spacy\n",
    "\n",
    "# --- 1. Param√®tres de Configuration et Pr√©paration spaCy ---\n",
    "\n",
    "# Chemins relatifs √† Plant_search/plant-search-engine/indexer/\n",
    "DOSSIER_PLANTES_JSON = \"../docs/Plantes\"\n",
    "DOSSIER_CONCEPTS_PDF = \"../docs/Concepts\"\n",
    "\n",
    "# FICHIER D'ENTR√âE POUR LE STOCK SCIENTIFIQUE\n",
    "FICHIER_STOCK_SCIENTIFIQUE = \"../docs/mot_scientifique/protected_terms.json\" \n",
    "\n",
    "# Fichiers de sortie (dans le dossier indexer/)\n",
    "FICHIER_INDEX_SORTIE = \"Base_Index.json\"\n",
    "FICHIER_LONGUEURS_SORTIE = \"document_lengths.json\"\n",
    "\n",
    "LANGUE = 'fr_core_news_sm'\n",
    "MAX_NGRAM_SCIENTIFIQUE = 4 # Longueur maximale des N-grammes √† v√©rifier\n",
    "\n",
    "# Chargement du mod√®le spaCy\n",
    "try:\n",
    "    nlp = spacy.load(LANGUE)\n",
    "except OSError:\n",
    "    print(f\"\\nüö® Erreur: Mod√®le spaCy '{LANGUE}' non trouv√©.\")\n",
    "    print(f\"Veuillez l'installer avec : python -m spacy download {LANGUE}\")\n",
    "    exit()\n",
    "\n",
    "# Stock des termes scientifiques charg√© au d√©marrage\n",
    "TERMES_SCIENTIFIQUES_STOCK = set()\n",
    "\n",
    "def charger_stock_scientifique(filepath: str) -> set:\n",
    "    \"\"\"\n",
    "    Charge le stock scientifique depuis un fichier JSON. \n",
    "    Les termes sont convertis en minuscules pour la comparaison.\n",
    "    \"\"\"\n",
    "    stock = set()\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ö†Ô∏è Avertissement: Fichier de stock scientifique '{filepath}' non trouv√©. Le stock est vide.\")\n",
    "        return stock\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            if isinstance(data, list):\n",
    "                stock.update({item.lower().strip() for item in data})\n",
    "            elif isinstance(data, dict):\n",
    "                stock.update({key.lower().strip() for key in data.keys()})\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Avertissement: Format du fichier '{filepath}' non reconnu.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du chargement du stock scientifique: {e}\")\n",
    "        return set()\n",
    "    \n",
    "    print(f\"‚úÖ Stock de {len(stock)} termes scientifiques/sp√©ciaux charg√©s.\")\n",
    "    return stock\n",
    "\n",
    "# Charger le stock d√®s que possible\n",
    "TERMES_SCIENTIFIQUES_STOCK = charger_stock_scientifique(FICHIER_STOCK_SCIENTIFIQUE)\n",
    "\n",
    "\n",
    "# --- 2. Fonctions d'Extraction et de Traitement du Texte ---\n",
    "\n",
    "def extraire_texte_json_avec_termes_separes(filepath: str) -> tuple[str, list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Extrait le texte g√©n√©ral, les termes num√©riques, et les termes Darija.\n",
    "    Les termes Darija sont extraits directement (tels quels, en minuscule).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur d'extraction/lecture JSON √† {filepath}: {e}\")\n",
    "        return \"\", [], []\n",
    "        \n",
    "    texte_general = []\n",
    "    termes_nombres = []\n",
    "    termes_darija_directs = [] # Pour les termes Darija (pris tels quels)\n",
    "    \n",
    "    def parcourir_dict(d):\n",
    "        for key, value in d.items():\n",
    "            if key in ('url', 'urls', 'galerie_images', 'id'):\n",
    "                continue\n",
    "                \n",
    "            # GESTION SP√âCIALE DES NOMS DARIJA (Indexation Directe)\n",
    "            if key == 'noms_darija' and isinstance(value, list):\n",
    "                # Ajout des termes Darija en minuscules, tels quels, sans lemmatisation\n",
    "                termes_darija_directs.extend([t.lower().strip() for t in value if t])\n",
    "                continue # NE PAS AJOUTER AU TEXTE G√âN√âRAL\n",
    "\n",
    "            # Le reste (y compris 'nom_scientifique') est ajout√© au texte g√©n√©ral\n",
    "            if isinstance(value, dict):\n",
    "                parcourir_dict(value)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        parcourir_dict(item)\n",
    "                    elif isinstance(item, str):\n",
    "                        texte_general.append(item)\n",
    "            elif isinstance(value, str):\n",
    "                texte_general.append(value)\n",
    "    \n",
    "    parcourir_dict(data)\n",
    "    \n",
    "    texte_complet = ' '.join(texte_general)\n",
    "    \n",
    "    # 3. Extraction des Nombres (sans leurs unit√©s)\n",
    "    nombres = re.findall(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', texte_complet, re.IGNORECASE)\n",
    "    \n",
    "    for nombre, unite in nombres:\n",
    "        terme_nombre = nombre.replace(',', '.') \n",
    "        termes_nombres.append(terme_nombre)\n",
    "    \n",
    "    # Nettoyer le texte g√©n√©ral des chiffres pour le traitement NLP\n",
    "    texte_nettoye = re.sub(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', ' ', texte_complet, flags=re.IGNORECASE)\n",
    "        \n",
    "    return texte_nettoye, termes_nombres, termes_darija_directs\n",
    "\n",
    "def extraire_texte_pdf(filepath: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    texte = \"\"\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            reader = pypdf.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                texte += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors de l'extraction de texte du PDF {filepath}. Erreur: {e}\")\n",
    "        return \"\"\n",
    "    return texte\n",
    "\n",
    "def identifier_termes_scientifiques(tokens: list, stock_scientifique: set, max_n: int) -> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Identifie et extrait les termes scientifiques/sp√©ciaux multi-mots (N-grammes)\n",
    "    en comparant le texte tokenis√© au stock.\n",
    "    \"\"\"\n",
    "    mots_scientifiques = []\n",
    "    indices_captures = set() \n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i in indices_captures:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        terme_trouve = False\n",
    "        \n",
    "        # Teste les N-grammes de la plus grande taille √† la plus petite (ex: 4, 3, 2, 1)\n",
    "        for n in range(min(max_n, len(tokens) - i), 0, -1):\n",
    "            \n",
    "            ngram_tokens = tokens[i:i+n]\n",
    "            \n",
    "            # Concat√©nation des tokens pour la comparaison\n",
    "            ngram_candidat = \" \".join(ngram_tokens)\n",
    "            ngram_candidat_lower = ngram_candidat.lower().strip()\n",
    "            \n",
    "            if ngram_candidat_lower in stock_scientifique:\n",
    "                # Terme scientifique/sp√©cial trouv√© ! On l'ajoute tel quel.\n",
    "                mots_scientifiques.append(ngram_candidat_lower)\n",
    "                \n",
    "                # Marquer tous les tokens de ce N-gramme comme captur√©s\n",
    "                for j in range(n):\n",
    "                    indices_captures.add(i + j)\n",
    "                \n",
    "                terme_trouve = True\n",
    "                i += n \n",
    "                break\n",
    "        \n",
    "        if not terme_trouve:\n",
    "            i += 1\n",
    "            \n",
    "    # Extraction des tokens restants (mots g√©n√©raux)\n",
    "    tokens_restants = [tokens[i] for i in range(len(tokens)) if i not in indices_captures]\n",
    "    \n",
    "    return mots_scientifiques, tokens_restants\n",
    "\n",
    "\n",
    "def normaliser_texte_lemmatisation_filtree(tokens_restants: list) -> list[str]:\n",
    "    \"\"\"\n",
    "    Lemmatisation stricte des tokens restants (mots g√©n√©raux). \n",
    "    Applique le filtrage : minuscules, enl√®ve ponctuation, stop words, symboles, puis lemmatise.\n",
    "    \"\"\"\n",
    "    texte_restant = \" \".join(tokens_restants) \n",
    "    doc = nlp(texte_restant.lower())\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "    mots_normalises = []\n",
    "    \n",
    "    for token in doc:\n",
    "        lemma_text = token.lemma_.lower().strip()\n",
    "        \n",
    "        # Application du filtrage strict pour les mots g√©n√©raux\n",
    "        if (\n",
    "            not token.is_punct and      # Enl√®ve la ponctuation\n",
    "            token.is_alpha and          # Enl√®ve les symboles et caract√®res non alphab√©tiques\n",
    "            not token.is_stop and       # Enl√®ve les stop words\n",
    "            len(lemma_text) > 1         # Longueur minimale du lemme\n",
    "        ):\n",
    "            mots_normalises.append(lemma_text) \n",
    "            \n",
    "    return mots_normalises\n",
    "\n",
    "# --- 3. Classe Principale d'Indexation ---\n",
    "\n",
    "class IndexeurBotanique:\n",
    "    \n",
    "    def __init__(self, json_dir, pdf_dir):\n",
    "        self.json_dir = json_dir\n",
    "        self.pdf_dir = pdf_dir\n",
    "        # Structure TF simple: { 'mot': { 'document_id': fr√©quence } }\n",
    "        self.index = defaultdict(lambda: defaultdict(int))\n",
    "        self.doc_lengths = {}\n",
    "        \n",
    "    def _creer_doc_id(self, filename: str, doc_type: str) -> str:\n",
    "        \"\"\"Cr√©e l'ID du document en utilisant LE NOM DE FICHIER COMPLET (avec extension).\"\"\"\n",
    "        return filename\n",
    "\n",
    "    def parcourir_documents(self):\n",
    "        \"\"\"Parcourt et indexe TOUS les JSON et TOUS les PDF.\"\"\"\n",
    "        documents_a_indexer = []\n",
    "        \n",
    "        print(f\"Recherche des fichiers JSON dans '{self.json_dir}'...\")\n",
    "        if os.path.isdir(self.json_dir):\n",
    "            for filename in os.listdir(self.json_dir):\n",
    "                if filename.endswith(\".json\"):\n",
    "                    doc_id = self._creer_doc_id(filename, 'json')\n",
    "                    documents_a_indexer.append({'id': doc_id, 'type': 'json', 'path': os.path.join(self.json_dir, filename)})\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur: Dossier JSON '{self.json_dir}' introuvable.\")\n",
    "\n",
    "\n",
    "        print(f\"Recherche des fichiers PDF dans '{self.pdf_dir}'...\")\n",
    "        if os.path.isdir(self.pdf_dir):\n",
    "            for filename in os.listdir(self.pdf_dir):\n",
    "                if filename.endswith(\".pdf\"):\n",
    "                    doc_id = self._creer_doc_id(filename, 'pdf')\n",
    "                    documents_a_indexer.append({'id': doc_id, 'type': 'pdf', 'path': os.path.join(self.pdf_dir, filename)})\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur: Dossier PDF '{self.pdf_dir}' introuvable.\")\n",
    "                \n",
    "        print(f\"\\nD√©but de l'indexation de {len(documents_a_indexer)} documents...\")\n",
    "        \n",
    "        for doc_info in documents_a_indexer:\n",
    "             self.indexer_document(doc_info)\n",
    "        print(\"\\n--- Indexation Termin√©e ---\")\n",
    "\n",
    "    # La fonction parcourir_documents_test() est conserv√©e mais non utilis√©e dans le main\n",
    "    def parcourir_documents_test(self, nom_json: str, nom_pdf: str):\n",
    "         \"\"\"Parcourt et indexe seulement 1 JSON et 1 PDF sp√©cifiques pour le test.\"\"\"\n",
    "         # ... (votre impl√©mentation)\n",
    "\n",
    "    def indexer_document(self, doc_info):\n",
    "        \"\"\"Extrait, normalise et construit l'index TF pour un document unique.\"\"\"\n",
    "        doc_id = doc_info['id']\n",
    "        filepath = doc_info['path']\n",
    "        \n",
    "        mots_indexables = []\n",
    "        texte_brut_pour_tokenisation = \"\"\n",
    "        termes_nombres = []\n",
    "        termes_darija_directs = []\n",
    "\n",
    "        # 1. Extraction du texte, nombres et termes Darija\n",
    "        if doc_info['type'] == 'json':\n",
    "            texte_brut_pour_tokenisation, termes_nombres, termes_darija_directs = extraire_texte_json_avec_termes_separes(filepath)\n",
    "            \n",
    "        elif doc_info['type'] == 'pdf':\n",
    "            texte_complet_pdf = extraire_texte_pdf(filepath)\n",
    "            if not texte_complet_pdf: return \n",
    "            \n",
    "            # Extraction des Nombres (PDF)\n",
    "            nombres = re.findall(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', texte_complet_pdf, re.IGNORECASE)\n",
    "            for nombre, unite in nombres:\n",
    "                 termes_nombres.append(nombre.replace(',', '.')) \n",
    "            \n",
    "            # Nettoyage du texte g√©n√©ral des chiffres pour le traitement NLP\n",
    "            texte_brut_pour_tokenisation = re.sub(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', ' ', texte_complet_pdf, flags=re.IGNORECASE)\n",
    "\n",
    "        # 2. AJOUT DIRECT des Termes (Nombres et Darija)\n",
    "        mots_indexables.extend(termes_nombres)\n",
    "        mots_indexables.extend(termes_darija_directs) \n",
    "        \n",
    "        # 3. Tokenisation pour la reconnaissance des N-grammes\n",
    "        doc = nlp(texte_brut_pour_tokenisation)\n",
    "        tokens_bruts = [token.text for token in doc if not token.is_space]\n",
    "        \n",
    "        # 4. Reconnaissance des Termes Scientifiques (non lemmatis√©s)\n",
    "        mots_scientifiques, tokens_restants = identifier_termes_scientifiques(\n",
    "            tokens_bruts, TERMES_SCIENTIFIQUES_STOCK, MAX_NGRAM_SCIENTIFIQUE\n",
    "        )\n",
    "        mots_indexables.extend(mots_scientifiques) \n",
    "        \n",
    "        # 5. Lemmatisation des Mots G√©n√©raux Restants\n",
    "        mots_lemmatises = normaliser_texte_lemmatisation_filtree(tokens_restants)\n",
    "        mots_indexables.extend(mots_lemmatises)\n",
    "\n",
    "        if not mots_indexables: return\n",
    "\n",
    "        # 6. Enregistrement de la longueur et construction de l'index\n",
    "        self.doc_lengths[doc_id] = len(mots_indexables)\n",
    "        \n",
    "        for mot in mots_indexables:\n",
    "            if mot:\n",
    "                self.index[mot][doc_id] += 1 \n",
    "\n",
    "    def enregistrer_longueurs(self, output_file):\n",
    "        \"\"\"Enregistre les longueurs des documents.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.doc_lengths, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Longueurs des documents (Token Counts) enregistr√©es dans '{output_file}'.\")\n",
    "\n",
    "    def enregistrer_index(self, output_file):\n",
    "        \"\"\"Enregistre l'index invers√© final (Base d'Index) au format simplifi√©.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)\n",
    "        \n",
    "        index_a_sauvegarder = dict(self.index)\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(index_a_sauvegarder, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"‚úÖ Base d'Index finale ('{output_file}') enregistr√©e.\")\n",
    "\n",
    "\n",
    "# --- 4. Ex√©cution du Script Principal (Mode Production) ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- D√©marrage de l'Indexation Compl√®te (M√©thode Avanc√©e) ---\")\n",
    "\n",
    "    # Le stock scientifique est d√©j√† charg√© au d√©but du script\n",
    "\n",
    "    # 1. Initialisation de l'Indexeur\n",
    "    indexeur = IndexeurBotanique(\n",
    "        json_dir=DOSSIER_PLANTES_JSON,\n",
    "        pdf_dir=DOSSIER_CONCEPTS_PDF\n",
    "    )\n",
    "    \n",
    "    # 2. Ex√©cution de l'indexation sur TOUS les fichiers\n",
    "    indexeur.parcourir_documents()\n",
    "    \n",
    "    # 3. Enregistrement des Fichiers de Sortie\n",
    "    indexeur.enregistrer_longueurs(FICHIER_LONGUEURS_SORTIE)\n",
    "    indexeur.enregistrer_index(FICHIER_INDEX_SORTIE)\n",
    "\n",
    "    print(\"\\n--- Processus d'Indexation Complet Termin√© ---\")\n",
    "    print(f\"L'index final ('{FICHIER_INDEX_SORTIE}') et les longueurs ('{FICHIER_LONGUEURS_SORTIE}') sont g√©n√©r√©s dans le dossier de l'indexeur.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf3f09",
   "metadata": {},
   "source": [
    "Ensuite genration des vocabulaires qui contient que les tokens pour les json et pour les pdf  separ√©s.\n",
    "pour un usage ulterieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c27d67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stock de 615 termes scientifiques/sp√©ciaux charg√©s.\n",
      "--- D√©marrage de l'Extraction S√©par√©e du Vocabulaire (Original & Nettoy√©) ---\n",
      "Recherche des fichiers JSON dans '../docs/Plantes'...\n",
      "Recherche des fichiers PDF dans '../docs/Concepts'...\n",
      "\n",
      "D√©but de l'extraction de 75 JSONs et 20 PDFs...\n",
      "  > Index√© JSON '98206.json' : 336 tokens.\n",
      "  > Index√© JSON '157344.json' : 199 tokens.\n",
      "  > Index√© JSON '104788.json' : 232 tokens.\n",
      "  > Index√© JSON '284473.json' : 259 tokens.\n",
      "  > Index√© JSON '211789.json' : 319 tokens.\n",
      "  > Index√© JSON '161718.json' : 280 tokens.\n",
      "  > Index√© JSON '158382.json' : 270 tokens.\n",
      "  > Index√© JSON '103942.json' : 224 tokens.\n",
      "  > Index√© JSON '186006.json' : 220 tokens.\n",
      "  > Index√© JSON '204705.json' : 376 tokens.\n",
      "  > Index√© JSON '5945.json' : 307 tokens.\n",
      "  > Index√© JSON '102667.json' : 415 tokens.\n",
      "  > Index√© JSON '109482.json' : 344 tokens.\n",
      "  > Index√© JSON '253369.json' : 299 tokens.\n",
      "  > Index√© JSON '78383.json' : 286 tokens.\n",
      "  > Index√© JSON '64135.json' : 253 tokens.\n",
      "  > Index√© JSON '97680.json' : 356 tokens.\n",
      "  > Index√© JSON '66545.json' : 223 tokens.\n",
      "  > Index√© JSON '934.json' : 219 tokens.\n",
      "  > Index√© JSON '98373.json' : 256 tokens.\n",
      "  > Index√© JSON '123023.json' : 271 tokens.\n",
      "  > Index√© JSON '200080.json' : 242 tokens.\n",
      "  > Index√© JSON '51968.json' : 243 tokens.\n",
      "  > Index√© JSON '78078.json' : 198 tokens.\n",
      "  > Index√© JSON '110975.json' : 327 tokens.\n",
      "  > Index√© JSON '51586.json' : 195 tokens.\n",
      "  > Index√© JSON '125085.json' : 302 tokens.\n",
      "  > Index√© JSON '168420.json' : 232 tokens.\n",
      "  > Index√© JSON '194712.json' : 163 tokens.\n",
      "  > Index√© JSON '98299.json' : 290 tokens.\n",
      "  > Index√© JSON '64847.json' : 435 tokens.\n",
      "  > Index√© JSON '95794.json' : 244 tokens.\n",
      "  > Index√© JSON '101391.json' : 221 tokens.\n",
      "  > Index√© JSON '265528.json' : 243 tokens.\n",
      "  > Index√© JSON '66568.json' : 213 tokens.\n",
      "  > Index√© JSON '164710.json' : 221 tokens.\n",
      "  > Index√© JSON '51508.json' : 246 tokens.\n",
      "  > Index√© JSON '161695.json' : 262 tokens.\n",
      "  > Index√© JSON '54862.json' : 261 tokens.\n",
      "  > Index√© JSON '127170.json' : 336 tokens.\n",
      "  > Index√© JSON '61646.json' : 281 tokens.\n",
      "  > Index√© JSON '265454.json' : 247 tokens.\n",
      "  > Index√© JSON '145097.json' : 277 tokens.\n",
      "  > Index√© JSON '55246.json' : 414 tokens.\n",
      "  > Index√© JSON '4170.json' : 300 tokens.\n",
      "  > Index√© JSON '153102.json' : 227 tokens.\n",
      "  > Index√© JSON '122295.json' : 222 tokens.\n",
      "  > Index√© JSON '6874.json' : 249 tokens.\n",
      "  > Index√© JSON '296871.json' : 199 tokens.\n",
      "  > Index√© JSON '217398.json' : 197 tokens.\n",
      "  > Index√© JSON '35939.json' : 293 tokens.\n",
      "  > Index√© JSON '264858.json' : 243 tokens.\n",
      "  > Index√© JSON '87441.json' : 235 tokens.\n",
      "  > Index√© JSON '35671.json' : 217 tokens.\n",
      "  > Index√© JSON '63441.json' : 229 tokens.\n",
      "  > Index√© JSON '29582.json' : 238 tokens.\n",
      "  > Index√© JSON '123309.json' : 366 tokens.\n",
      "  > Index√© JSON '161063.json' : 192 tokens.\n",
      "  > Index√© JSON '79591.json' : 343 tokens.\n",
      "  > Index√© JSON '69568.json' : 514 tokens.\n",
      "  > Index√© JSON '107333.json' : 255 tokens.\n",
      "  > Index√© JSON '194717.json' : 175 tokens.\n",
      "  > Index√© JSON '216107.json' : 167 tokens.\n",
      "  > Index√© JSON '385323.json' : 296 tokens.\n",
      "  > Index√© JSON '61640.json' : 243 tokens.\n",
      "  > Index√© JSON '383367.json' : 253 tokens.\n",
      "  > Index√© JSON '131870.json' : 322 tokens.\n",
      "  > Index√© JSON '204029.json' : 204 tokens.\n",
      "  > Index√© JSON '2164.json' : 228 tokens.\n",
      "  > Index√© JSON '165723.json' : 306 tokens.\n",
      "  > Index√© JSON '84725.json' : 236 tokens.\n",
      "  > Index√© JSON '117959.json' : 221 tokens.\n",
      "  > Index√© JSON '265222.json' : 220 tokens.\n",
      "  > Index√© JSON '67625.json' : 267 tokens.\n",
      "  > Index√© JSON '252096.json' : 190 tokens.\n",
      "  > Index√© PDF 'L'Humidit√© Ambiante.pdf' : 417 tokens.\n",
      "  > Index√© PDF 'Le Drainage et l'A√©ration du Sol.pdf' : 609 tokens.\n",
      "  > Index√© PDF 'Propagation et Reproduction V√©g√©tale.pdf' : 465 tokens.\n",
      "  > Index√© PDF 'Maladies Fongiques.pdf' : 379 tokens.\n",
      "  > Index√© PDF 'Arrosage.pdf' : 445 tokens.\n",
      "  > Index√© PDF 'Stress Hydrique _ Thermique _ Lumineux.pdf' : 347 tokens.\n",
      "  > Index√© PDF 'Croissance et D√©veloppement.pdf' : 353 tokens.\n",
      "  > Index√© PDF 'Taille et √âlagage V√©g√©tal.pdf' : 351 tokens.\n",
      "  > Index√© PDF 'pH du Sol .pdf' : 438 tokens.\n",
      "  > Index√© PDF 'Types de Sol.pdf' : 528 tokens.\n",
      "  > Index√© PDF 'Origine G√©ographique et Biomes.pdf' : 251 tokens.\n",
      "  > Index√© PDF 'Parasites Courants.pdf' : 323 tokens.\n",
      "  > Index√© PDF 'Cycle de Vie.pdf' : 227 tokens.\n",
      "  > Index√© PDF 'La Temp√©rature et le Climat.pdf' : 615 tokens.\n",
      "  > Index√© PDF 'Luminosit√© .pdf' : 558 tokens.\n",
      "  > Index√© PDF 'Technologies Horticoles Modernes.pdf' : 389 tokens.\n",
      "  > Index√© PDF 'Plantation.pdf' : 639 tokens.\n",
      "  > Index√© PDF 'Nutrition V√©g√©tale et les Engrais.pdf' : 588 tokens.\n",
      "  > Index√© PDF 'Toxicit√© V√©g√©tale.pdf' : 386 tokens.\n",
      "  > Index√© PDF 'Le Rempotage et le Surfa√ßage des Plantes.pdf' : 565 tokens.\n",
      "\n",
      "--- Extraction de Vocabulaire Termin√©e ---\n",
      "Vocabulaire Plantes (JSON) : 4673 termes uniques.\n",
      "Vocabulaire Concepts (PDF) : 3592 termes uniques.\n",
      "‚úÖ Vocabulaire des Concepts (Original) enregistr√© dans 'vocabulaire_concepts.json'.\n",
      "‚úÖ Vocabulaire des Plantes (Original) enregistr√© dans 'vocabulaire_plantes.json'.\n",
      "‚úÖ Vocabulaire des Concepts (NETTOY√â) enregistr√© dans 'vocabulaire_concepts_nettoye.json'.\n",
      "‚úÖ Vocabulaire des Plantes (NETTOY√â) enregistr√© dans 'vocabulaire_plantes_nettoye.json'.\n",
      "\n",
      "--- Processus d'Extraction de Vocabulaire Complet Termin√© ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pypdf\n",
    "import spacy\n",
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "# --- 1. Param√®tres de Configuration et Pr√©paration spaCy ---\n",
    "\n",
    "# Chemins relatifs √† Plant_search/plant-search-engine/indexer/\n",
    "DOSSIER_PLANTES_JSON = \"../docs/Plantes\"\n",
    "DOSSIER_CONCEPTS_PDF = \"../docs/Concepts\"\n",
    "\n",
    "# FICHIER D'ENTR√âE POUR LE STOCK SCIENTIFIQUE\n",
    "FICHIER_STOCK_SCIENTIFIQUE = \"../docs/mot_scientifique/protected_terms.json\" \n",
    "\n",
    "# Fichiers de sortie (versions Originales - avec underscores)\n",
    "FICHIER_VOCABULAIRE_CONCEPTS = \"vocabulaire_concepts.json\"\n",
    "FICHIER_VOCABULAIRE_PLANTES = \"vocabulaire_plantes.json\"\n",
    "\n",
    "# Fichiers de sortie (versions Nettoy√©es - sans caract√®res sp√©ciaux)\n",
    "FICHIER_VOCABULAIRE_CONCEPTS_NETTOYE = \"vocabulaire_concepts_nettoye.json\"\n",
    "FICHIER_VOCABULAIRE_PLANTES_NETTOYE = \"vocabulaire_plantes_nettoye.json\"\n",
    "\n",
    "\n",
    "LANGUE = 'fr_core_news_sm'\n",
    "MAX_NGRAM_SCIENTIFIQUE = 4 \n",
    "\n",
    "# Chargement du mod√®le spaCy\n",
    "try:\n",
    "    nlp = spacy.load(LANGUE)\n",
    "except OSError:\n",
    "    print(f\"\\nüö® Erreur: Mod√®le spaCy '{LANGUE}' non trouv√©.\")\n",
    "    print(f\"Veuillez l'installer avec : python -m spacy download {LANGUE}\")\n",
    "    exit()\n",
    "\n",
    "# Stock des termes scientifiques\n",
    "TERMES_SCIENTIFIQUES_STOCK = set()\n",
    "\n",
    "def charger_stock_scientifique(filepath: str) -> set:\n",
    "    \"\"\"\n",
    "    Charge le stock scientifique, convertit les termes en minuscules\n",
    "    et remplace les espaces par des underscores pour le format de token.\n",
    "    \"\"\"\n",
    "    stock = set()\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ö†Ô∏è Avertissement: Fichier de stock scientifique '{filepath}' non trouv√©. Le stock est vide.\")\n",
    "        return stock\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                # Les termes N-grammes sont stock√©s avec des underscores pour l'indexation\n",
    "                stock.update({item.lower().strip().replace(\" \", \"_\") for item in data})\n",
    "            elif isinstance(data, dict):\n",
    "                stock.update({key.lower().strip().replace(\" \", \"_\") for key in data.keys()})\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Avertissement: Format du fichier '{filepath}' non reconnu.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du chargement du stock scientifique: {e}\")\n",
    "        return set()\n",
    "    \n",
    "    print(f\"‚úÖ Stock de {len(stock)} termes scientifiques/sp√©ciaux charg√©s.\")\n",
    "    return stock\n",
    "\n",
    "# Charger le stock d√®s que possible\n",
    "TERMES_SCIENTIFIQUES_STOCK = charger_stock_scientifique(FICHIER_STOCK_SCIENTIFIQUE)\n",
    "\n",
    "\n",
    "# --- 2. Fonctions d'Extraction et de Traitement du Texte ---\n",
    "\n",
    "def extraire_texte_json_avec_termes_separes(filepath: str) -> Tuple[str, List[str], List[str]]:\n",
    "    \"\"\"Extrait texte g√©n√©ral, nombres, et termes Darija.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur d'extraction/lecture JSON √† {filepath}: {e}\")\n",
    "        return \"\", [], []\n",
    "        \n",
    "    texte_general = []\n",
    "    termes_nombres = []\n",
    "    termes_darija_directs = []\n",
    "    \n",
    "    def parcourir_dict(d):\n",
    "        for key, value in d.items():\n",
    "            if key in ('url', 'urls', 'galerie_images', 'id'):\n",
    "                continue\n",
    "                \n",
    "            if key == 'noms_darija' and isinstance(value, list):\n",
    "                # Les termes Darija sont index√©s directs (minuscules, underscore)\n",
    "                termes_darija_directs.extend([t.lower().strip().replace(\" \", \"_\") for t in value if t])\n",
    "                continue\n",
    "\n",
    "            if isinstance(value, dict):\n",
    "                parcourir_dict(value)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        parcourir_dict(item)\n",
    "                    elif isinstance(item, str):\n",
    "                        texte_general.append(item)\n",
    "            elif isinstance(value, str):\n",
    "                texte_general.append(value)\n",
    "    \n",
    "    parcourir_dict(data)\n",
    "    \n",
    "    texte_complet = ' '.join(texte_general)\n",
    "    \n",
    "    # Extraction des Nombres (sans leurs unit√©s)\n",
    "    nombres = re.findall(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', texte_complet, re.IGNORECASE)\n",
    "    \n",
    "    for nombre, unite in nombres:\n",
    "        # Stockage des nombres avec point '.' comme s√©parateur d√©cimal (si applicable)\n",
    "        terme_nombre = nombre.replace(',', '.') \n",
    "        termes_nombres.append(terme_nombre)\n",
    "    \n",
    "    # Nettoyer le texte g√©n√©ral des chiffres pour le traitement NLP\n",
    "    texte_nettoye = re.sub(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', ' ', texte_complet, flags=re.IGNORECASE)\n",
    "        \n",
    "    return texte_nettoye, termes_nombres, termes_darija_directs\n",
    "\n",
    "\n",
    "def extraire_texte_pdf(filepath: str) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    texte = \"\"\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            reader = pypdf.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                texte += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors de l'extraction de texte du PDF {filepath}. Erreur: {e}\")\n",
    "        return \"\"\n",
    "    return texte\n",
    "\n",
    "def identifier_termes_scientifiques(tokens: list, stock_scientifique: set, max_n: int) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Identifie et extrait les termes scientifiques/sp√©ciaux multi-mots (N-grammes)\n",
    "    en comparant le texte tokenis√© au stock.\n",
    "    Les termes sont renvoy√©s avec des underscores (comme dans le stock).\n",
    "    \"\"\"\n",
    "    mots_scientifiques = []\n",
    "    indices_captures = set() \n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i in indices_captures:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        terme_trouve = False\n",
    "        \n",
    "        for n in range(min(max_n, len(tokens) - i), 0, -1):\n",
    "            \n",
    "            ngram_tokens = tokens[i:i+n]\n",
    "            \n",
    "            # Concat√©nation des tokens et conversion en underscore pour la comparaison au stock\n",
    "            ngram_candidat = \" \".join(ngram_tokens)\n",
    "            ngram_candidat_stock = ngram_candidat.lower().strip().replace(\" \", \"_\") \n",
    "            \n",
    "            if ngram_candidat_stock in stock_scientifique:\n",
    "                mots_scientifiques.append(ngram_candidat_stock)\n",
    "                \n",
    "                for j in range(n):\n",
    "                    indices_captures.add(i + j)\n",
    "                \n",
    "                terme_trouve = True\n",
    "                i += n \n",
    "                break\n",
    "        \n",
    "        if not terme_trouve:\n",
    "            i += 1\n",
    "            \n",
    "    tokens_restants = [tokens[i] for i in range(len(tokens)) if i not in indices_captures]\n",
    "    \n",
    "    return mots_scientifiques, tokens_restants\n",
    "\n",
    "\n",
    "def normaliser_texte_lemmatisation_filtree(tokens_restants: list) -> List[str]:\n",
    "    \"\"\"\n",
    "    Lemmatisation stricte des tokens restants (mots g√©n√©raux). \n",
    "    \"\"\"\n",
    "    texte_restant = \" \".join(tokens_restants) \n",
    "    doc = nlp(texte_restant.lower())\n",
    "    mots_normalises = []\n",
    "    \n",
    "    for token in doc:\n",
    "        lemma_text = token.lemma_.lower().strip()\n",
    "        \n",
    "        if (\n",
    "            not token.is_punct and      \n",
    "            token.is_alpha and          \n",
    "            not token.is_stop and       \n",
    "            len(lemma_text) > 1         \n",
    "        ):\n",
    "            mots_normalises.append(lemma_text) \n",
    "            \n",
    "    return mots_normalises\n",
    "\n",
    "def normaliser_chaine_pour_sauvegarde(mot: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalise une cha√Æne en la d√©barrassant des s√©parateurs non alphab√©tiques\n",
    "    et des caract√®res sp√©ciaux (y compris '_', '√ó', '-', etc.).\n",
    "    Note: Cela retire les chiffres car r'[^a-z]' est utilis√©.\n",
    "    \"\"\"\n",
    "    mot = mot.lower().strip()\n",
    "    # Retire tous les caract√®res qui ne sont pas des lettres\n",
    "    mot_nettoye = re.sub(r'[^a-z]', '', mot) \n",
    "    return mot_nettoye\n",
    "\n",
    "# --- 3. Classe Principale d'Extraction de Vocabulaire ---\n",
    "\n",
    "class ExtracteurVocabulaire:\n",
    "    \n",
    "    def __init__(self, json_dir, pdf_dir):\n",
    "        self.json_dir = json_dir\n",
    "        self.pdf_dir = pdf_dir\n",
    "        # Stockage des tokens uniques pour chaque type\n",
    "        self.vocabulaire_plantes = set() # Pour JSON\n",
    "        self.vocabulaire_concepts = set() # Pour PDF\n",
    "        \n",
    "    def _indexer_un_document(self, doc_info: dict) -> Set[str]:\n",
    "        \"\"\"Extrait, normalise et retourne l'ensemble de tokens indexables pour un document unique.\"\"\"\n",
    "        \n",
    "        mots_indexables: List[str] = []\n",
    "        texte_brut_pour_tokenisation = \"\"\n",
    "        termes_nombres = []\n",
    "        termes_darija_directs = []\n",
    "        \n",
    "        # 1. Extraction du texte, nombres et termes Darija\n",
    "        if doc_info['type'] == 'json':\n",
    "            texte_brut_pour_tokenisation, termes_nombres, termes_darija_directs = extraire_texte_json_avec_termes_separes(doc_info['path'])\n",
    "            \n",
    "        elif doc_info['type'] == 'pdf':\n",
    "            texte_complet_pdf = extraire_texte_pdf(doc_info['path'])\n",
    "            if not texte_complet_pdf: return set()\n",
    "            \n",
    "            # Extraction des Nombres (PDF)\n",
    "            nombres = re.findall(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', texte_complet_pdf, re.IGNORECASE)\n",
    "            for nombre, unite in nombres:\n",
    "                 termes_nombres.append(nombre.replace(',', '.')) \n",
    "            \n",
    "            # Nettoyage du texte g√©n√©ral des chiffres pour le traitement NLP\n",
    "            texte_brut_pour_tokenisation = re.sub(r'(\\d+[\\.,]\\d+|\\d+)\\s*(%|ml|g|cm|mm|m|l)?', ' ', texte_complet_pdf, flags=re.IGNORECASE)\n",
    "\n",
    "        # 2. AJOUT DIRECT des Termes (Nombres et Darija)\n",
    "        # Note: Les nombres sont conserv√©s, si l'on veut les supprimer, il faudrait les filtrer ici.\n",
    "        mots_indexables.extend(termes_nombres)\n",
    "        mots_indexables.extend(termes_darija_directs) \n",
    "        \n",
    "        # 3. Tokenisation pour la reconnaissance des N-grammes\n",
    "        doc = nlp(texte_brut_pour_tokenisation)\n",
    "        tokens_bruts = [token.text for token in doc if not token.is_space]\n",
    "        \n",
    "        # 4. Reconnaissance des Termes Scientifiques (N-grammes prot√©g√©s)\n",
    "        mots_scientifiques, tokens_restants = identifier_termes_scientifiques(\n",
    "            tokens_bruts, TERMES_SCIENTIFIQUES_STOCK, MAX_NGRAM_SCIENTIFIQUE\n",
    "        )\n",
    "        mots_indexables.extend(mots_scientifiques) \n",
    "        \n",
    "        # 5. Lemmatisation des Mots G√©n√©raux Restants\n",
    "        mots_lemmatises = normaliser_texte_lemmatisation_filtree(tokens_restants)\n",
    "        mots_indexables.extend(mots_lemmatises)\n",
    "\n",
    "        return set(mots_indexables) # Retourne l'ensemble unique de tokens pour ce document\n",
    "        \n",
    "    def parcourir_documents(self):\n",
    "        \"\"\"Parcourt tous les documents et agr√®ge le vocabulaire.\"\"\"\n",
    "        \n",
    "        print(f\"Recherche des fichiers JSON dans '{self.json_dir}'...\")\n",
    "        json_fichiers = []\n",
    "        if os.path.isdir(self.json_dir):\n",
    "            json_fichiers = [\n",
    "                {'id': f, 'type': 'json', 'path': os.path.join(self.json_dir, f)} \n",
    "                for f in os.listdir(self.json_dir) if f.endswith(\".json\")\n",
    "            ]\n",
    "        \n",
    "        print(f\"Recherche des fichiers PDF dans '{self.pdf_dir}'...\")\n",
    "        pdf_fichiers = []\n",
    "        if os.path.isdir(self.pdf_dir):\n",
    "            pdf_fichiers = [\n",
    "                {'id': f, 'type': 'pdf', 'path': os.path.join(self.pdf_dir, f)}\n",
    "                for f in os.listdir(self.pdf_dir) if f.endswith(\".pdf\")\n",
    "            ]\n",
    "        \n",
    "        print(f\"\\nD√©but de l'extraction de {len(json_fichiers)} JSONs et {len(pdf_fichiers)} PDFs...\")\n",
    "\n",
    "        # A. Traitement des JSONs (Plantes)\n",
    "        for doc_info in json_fichiers:\n",
    "            tokens = self._indexer_un_document(doc_info)\n",
    "            self.vocabulaire_plantes.update(tokens)\n",
    "            print(f\"  > Index√© JSON '{doc_info['id']}' : {len(tokens)} tokens.\")\n",
    "\n",
    "        # B. Traitement des PDFs (Concepts)\n",
    "        for doc_info in pdf_fichiers:\n",
    "            tokens = self._indexer_un_document(doc_info)\n",
    "            self.vocabulaire_concepts.update(tokens)\n",
    "            print(f\"  > Index√© PDF '{doc_info['id']}' : {len(tokens)} tokens.\")\n",
    "\n",
    "        print(\"\\n--- Extraction de Vocabulaire Termin√©e ---\")\n",
    "        print(f\"Vocabulaire Plantes (JSON) : {len(self.vocabulaire_plantes)} termes uniques.\")\n",
    "        print(f\"Vocabulaire Concepts (PDF) : {len(self.vocabulaire_concepts)} termes uniques.\")\n",
    "\n",
    "    def enregistrer_vocabulaire(self):\n",
    "        \"\"\"Enregistre les deux ensembles de vocabulaire dans des fichiers JSON (VERSION ORIGINALE).\"\"\"\n",
    "        \n",
    "        # Enregistrement du vocabulaire des Concepts (PDF)\n",
    "        with open(FICHIER_VOCABULAIRE_CONCEPTS, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sorted(list(self.vocabulaire_concepts)), f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Vocabulaire des Concepts (Original) enregistr√© dans '{FICHIER_VOCABULAIRE_CONCEPTS}'.\")\n",
    "\n",
    "        # Enregistrement du vocabulaire des Plantes (JSON)\n",
    "        with open(FICHIER_VOCABULAIRE_PLANTES, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sorted(list(self.vocabulaire_plantes)), f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Vocabulaire des Plantes (Original) enregistr√© dans '{FICHIER_VOCABULAIRE_PLANTES}'.\")\n",
    "\n",
    "    def enregistrer_vocabulaire_nettoye(self):\n",
    "        \"\"\"Enregistre les deux ensembles de vocabulaire dans des fichiers JSON (VERSION NETTOY√âE).\"\"\"\n",
    "        \n",
    "        def nettoyer_et_collecter(vocab_set: Set[str]) -> List[str]:\n",
    "            mots_nettoyes = set()\n",
    "            for mot in vocab_set:\n",
    "                mot_nettoye = normaliser_chaine_pour_sauvegarde(mot)\n",
    "                # Ajoute uniquement les mots non vides apr√®s nettoyage (ex: si le token √©tait un simple chiffre, il deviendrait vide)\n",
    "                if mot_nettoye: \n",
    "                    mots_nettoyes.add(mot_nettoye)\n",
    "            # Retourne la liste tri√©e des mots uniques nettoy√©s\n",
    "            return sorted(list(mots_nettoyes))\n",
    "\n",
    "        # A. Nettoyage et enregistrement des Concepts (PDF)\n",
    "        vocab_concepts_net = nettoyer_et_collecter(self.vocabulaire_concepts)\n",
    "        with open(FICHIER_VOCABULAIRE_CONCEPTS_NETTOYE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab_concepts_net, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Vocabulaire des Concepts (NETTOY√â) enregistr√© dans '{FICHIER_VOCABULAIRE_CONCEPTS_NETTOYE}'.\")\n",
    "\n",
    "        # B. Nettoyage et enregistrement des Plantes (JSON)\n",
    "        vocab_plantes_net = nettoyer_et_collecter(self.vocabulaire_plantes)\n",
    "        with open(FICHIER_VOCABULAIRE_PLANTES_NETTOYE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab_plantes_net, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Vocabulaire des Plantes (NETTOY√â) enregistr√© dans '{FICHIER_VOCABULAIRE_PLANTES_NETTOYE}'.\")\n",
    "\n",
    "\n",
    "# --- 4. Ex√©cution du Script Principal ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- D√©marrage de l'Extraction S√©par√©e du Vocabulaire (Original & Nettoy√©) ---\")\n",
    "\n",
    "    # 1. Initialisation de l'Extracteur\n",
    "    extracteur = ExtracteurVocabulaire(\n",
    "        json_dir=DOSSIER_PLANTES_JSON,\n",
    "        pdf_dir=DOSSIER_CONCEPTS_PDF\n",
    "    )\n",
    "    \n",
    "    # 2. Ex√©cution de l'extraction\n",
    "    extracteur.parcourir_documents()\n",
    "    \n",
    "    # 3. Enregistrement des Fichiers de Sortie\n",
    "    extracteur.enregistrer_vocabulaire() # Version avec s√©parateurs (_)\n",
    "    extracteur.enregistrer_vocabulaire_nettoye() # Version sans s√©parateurs\n",
    "\n",
    "    print(\"\\n--- Processus d'Extraction de Vocabulaire Complet Termin√© ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca45a80",
   "metadata": {},
   "source": [
    "je teste ici si c'est vocabulaire sont dans la base d'index, comme ca je suis sur que ces tokens forment mon index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82dded4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- D√©marrage de la V√©rification de Coh√©rence (Vocabulaire Nettoy√© vs Index) ---\n",
      "\n",
      "‚úÖ Total de mots dans l'Index Nettoy√© : 6479\n",
      "‚úÖ Total de mots dans le Vocabulaire Global Nettoy√© : 6478\n",
      "\n",
      "\n",
      "=== I. Vocabulaire Global Nettoy√© ABSENT de l'Index Nettoy√© ===\n",
      "‚úÖ Coh√©rence totale : Tous les mots de 'Vocabulaire Global Nettoy√©' sont pr√©sents dans 'Base d'Index Nettoy√©e'.\n",
      "\n",
      "\n",
      "=== II. Base d'Index Nettoy√©e ABSENTE du Vocabulaire Global Nettoy√© ===\n",
      "‚ùå 1 mot(s) dans 'Base d'Index Nettoy√©e' sont ABSENTS de 'Vocabulaire Global Nettoy√©'.\n",
      "  > Aper√ßu des premiers mots : ['']\n",
      "  > Ce sont des diff√©rences fondamentales (lemmes ou tokens non appari√©s).\n",
      "\n",
      "üîé **Diagnostic des Mots Manquants dans le Vocabulaire :**\n",
      "  Le probl√®me pourrait venir :\n",
      "  1. D'un filtrage trop agressif (ex: suppression des chiffres/nombres dans le vocabulaire nettoy√©).\n",
      "  2. De mots tr√®s courts ou de stop-words qui ont √©t√© r√©introduits par l'Indexeur mais pas par l'Extracteur de vocabulaire.\n",
      "\n",
      "--- Processus de V√©rification Termin√© ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Set, Dict, List\n",
    "\n",
    "# --- 1. Chemins des Fichiers ---\n",
    "\n",
    "# üö® Assurez-vous que ces chemins sont corrects\n",
    "FICHIER_INDEX_SORTIE = \"Base_Index.json\"\n",
    "# Nouveaux fichiers de vocabulaire √† tester\n",
    "FICHIER_VOCABULAIRE_CONCEPTS_NETTOYE = \"vocabulaire_concepts_nettoye.json\"\n",
    "FICHIER_VOCABULAIRE_PLANTES_NETTOYE = \"vocabulaire_plantes_nettoye.json\"\n",
    "\n",
    "# --- 2. Fonctions Utilitaires ---\n",
    "\n",
    "def charger_vocabulaire(filepath: str, type_fichier: str) -> Set[str]:\n",
    "    \"\"\"Charge un fichier JSON et retourne l'ensemble (Set) de ses mots.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ùå ERREUR : Fichier {type_fichier} non trouv√© √† {filepath}.\")\n",
    "        return set()\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            if isinstance(data, list):\n",
    "                # Vocabulaire nettoy√© : liste de mots\n",
    "                return set(data)\n",
    "            \n",
    "            elif isinstance(data, dict):\n",
    "                # Base d'Index : dictionnaire (cl√©s = mots)\n",
    "                return set(data.keys())\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ùå ERREUR : Format du fichier {type_fichier} ({filepath}) inattendu.\")\n",
    "                return set()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR de lecture : {e}\")\n",
    "        return set()\n",
    "\n",
    "def normaliser_chaine_pour_comparaison(mot: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalise une cha√Æne pour la comparaison en la d√©barrassant de la plupart\n",
    "    des s√©parateurs et des caract√®res non alphab√©tiques.\n",
    "    Ceci est la fonction de \"nettoyage\" appliqu√©e √† la Base d'Index pour le test.\n",
    "    \"\"\"\n",
    "    mot = mot.lower().strip()\n",
    "    # Retire tous les caract√®res qui ne sont pas des lettres\n",
    "    mot_nettoye = re.sub(r'[^a-z]', '', mot)\n",
    "    return mot_nettoye\n",
    "\n",
    "def effectuer_test(set_a: Set[str], set_b: Set[str], nom_a: str, nom_b: str):\n",
    "    \"\"\"Effectue et affiche le test de diff√©rence A - B.\"\"\"\n",
    "    mots_manquants = set_a.difference(set_b)\n",
    "    \n",
    "    if mots_manquants:\n",
    "        print(f\"‚ùå {len(mots_manquants)} mot(s) dans '{nom_a}' sont ABSENTS de '{nom_b}'.\")\n",
    "        mots_a_afficher = sorted(list(mots_manquants))[:20]\n",
    "        print(f\"  > Aper√ßu des premiers mots : {mots_a_afficher}\")\n",
    "        print(\"  > Ce sont des diff√©rences fondamentales (lemmes ou tokens non appari√©s).\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Coh√©rence totale : Tous les mots de '{nom_a}' sont pr√©sents dans '{nom_b}'.\")\n",
    "    return mots_manquants\n",
    "\n",
    "# --- 3. Script Principal de V√©rification ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- D√©marrage de la V√©rification de Coh√©rence (Vocabulaire Nettoy√© vs Index) ---\")\n",
    "\n",
    "    # 1. Chargement des donn√©es\n",
    "    mots_concepts_net = charger_vocabulaire(FICHIER_VOCABULAIRE_CONCEPTS_NETTOYE, \"Vocabulaire Concepts Nettoy√©\")\n",
    "    mots_plantes_net = charger_vocabulaire(FICHIER_VOCABULAIRE_PLANTES_NETTOYE, \"Vocabulaire Plantes Nettoy√©\")\n",
    "    mots_index_brut = charger_vocabulaire(FICHIER_INDEX_SORTIE, \"Base d'Index\")\n",
    "\n",
    "    if not (mots_index_brut and mots_concepts_net and mots_plantes_net):\n",
    "        print(\"\\n‚ùå Arr√™t en raison d'un fichier manquant ou illisible.\")\n",
    "        exit()\n",
    "    \n",
    "    # 2. Pr√©paration de la Base d'Index pour la comparaison\n",
    "    # On normalise les mots de la Base d'Index (qui contiennent encore des espaces, etc.)\n",
    "    mots_index_net = {normaliser_chaine_pour_comparaison(mot) for mot in mots_index_brut}\n",
    "\n",
    "    # 3. Cr√©ation du Vocabulaire Global Nettoy√©\n",
    "    mots_vocabulaire_global_net = mots_concepts_net.union(mots_plantes_net)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total de mots dans l'Index Nettoy√© : {len(mots_index_net)}\")\n",
    "    print(f\"‚úÖ Total de mots dans le Vocabulaire Global Nettoy√© : {len(mots_vocabulaire_global_net)}\")\n",
    "\n",
    "\n",
    "    # ====================================================================\n",
    "    # A. TEST 1 : Vocabulaire Nettoy√© -> Index Nettoy√©\n",
    "    # (Doit √™tre proche de z√©ro, car la divergence de ponctuation est r√©solue)\n",
    "    # ====================================================================\n",
    "    \n",
    "    print(\"\\n\\n=== I. Vocabulaire Global Nettoy√© ABSENT de l'Index Nettoy√© ===\")\n",
    "    \n",
    "    # Test Vocabulaire Global\n",
    "    manquants_vocab_net = effectuer_test(mots_vocabulaire_global_net, mots_index_net, \"Vocabulaire Global Nettoy√©\", \"Base d'Index Nettoy√©e\")\n",
    "\n",
    "    # ====================================================================\n",
    "    # B. TEST 2 : Index Nettoy√© -> Vocabulaire Global Nettoy√© (Test Inverse)\n",
    "    # (V√©rifie si le Vocabulaire a rat√© des mots, m√™me apr√®s nettoyage)\n",
    "    # ====================================================================\n",
    "    \n",
    "    print(\"\\n\\n=== II. Base d'Index Nettoy√©e ABSENTE du Vocabulaire Global Nettoy√© ===\")\n",
    "    \n",
    "    manquants_index_net = effectuer_test(mots_index_net, mots_vocabulaire_global_net, \"Base d'Index Nettoy√©e\", \"Vocabulaire Global Nettoy√©\")\n",
    "    \n",
    "    if len(manquants_index_net) > 0:\n",
    "        print(\"\\nüîé **Diagnostic des Mots Manquants dans le Vocabulaire :**\")\n",
    "        print(\"  Le probl√®me pourrait venir :\")\n",
    "        print(\"  1. D'un filtrage trop agressif (ex: suppression des chiffres/nombres dans le vocabulaire nettoy√©).\")\n",
    "        print(\"  2. De mots tr√®s courts ou de stop-words qui ont √©t√© r√©introduits par l'Indexeur mais pas par l'Extracteur de vocabulaire.\")\n",
    "\n",
    "    print(\"\\n--- Processus de V√©rification Termin√© ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71497bd",
   "metadata": {},
   "source": [
    "le teste montre que c'est compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "038bd27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- D√©marrage de la Reconstruction du Vocabulaire depuis l'Index ---\n",
      "‚úÖ Base d'Index charg√©e. Contient 6748 tokens uniques.\n",
      "\n",
      "--- R√©sultat du Tri ---\n",
      "Total des tokens uniques dans l'Index : 6748\n",
      "Tokens class√©s comme 'Plantes' (JSON) : 4673\n",
      "Tokens class√©s comme 'Concepts' (PDF) : 3592\n",
      "‚úÖ Vocabulaire Plantes (JSON) enregistr√© dans 'token_plante.json' (4673 mots).\n",
      "‚úÖ Vocabulaire Concepts (PDF) enregistr√© dans 'token_concept.json' (3592 mots).\n",
      "\n",
      "--- Reconstruction du Vocabulaire bas√©e sur l'Index Termin√©e ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Set, Dict, List, Tuple\n",
    "\n",
    "# --- 1. Chemins des Fichiers ---\n",
    "\n",
    "FICHIER_INDEX_SORTIE = \"Base_Index.json\"\n",
    "# Nouveaux fichiers de sortie\n",
    "FICHIER_TOKEN_PLANTE = \"token_plante.json\"\n",
    "FICHIER_TOKEN_CONCEPT = \"token_concept.json\"\n",
    "\n",
    "# D√©finition des pr√©fixes ou extensions pour identifier les types de documents\n",
    "PREFIXE_PLANTE = \".json\"  # ID de documents JSON\n",
    "PREFIXE_CONCEPT = \".pdf\" # ID de documents PDF\n",
    "\n",
    "# --- 2. Fonction de Chargement ---\n",
    "\n",
    "def charger_base_index(filepath: str) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"Charge la Base d'Index Invers√©.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ùå ERREUR : Fichier Base d'Index non trouv√© √† {filepath}. Annulation.\")\n",
    "        return {}\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, dict):\n",
    "                print(f\"‚úÖ Base d'Index charg√©e. Contient {len(data)} tokens uniques.\")\n",
    "                # La structure attendue est Dict[token, Dict[doc_id, freq]]\n",
    "                return data\n",
    "            else:\n",
    "                print(\"‚ùå ERREUR : Le fichier Base d'Index n'est pas un dictionnaire.\")\n",
    "                return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ùå ERREUR JSON : Le fichier Base d'Index n'est pas un JSON valide.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR de lecture : {e}\")\n",
    "        return {}\n",
    "\n",
    "# --- 3. Fonction d'Extraction et de Tri (CORRIG√âE) ---\n",
    "\n",
    "def extraire_et_trier_tokens(base_index: Dict[str, Dict[str, int]]) -> Tuple[Set[str], Set[str]]:\n",
    "    \"\"\"\n",
    "    Parcourt l'index (cl√©=token, valeur=Dict[doc_id, freq]) et trie chaque token \n",
    "    dans les ensembles Plantes et Concepts.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens_plantes: Set[str] = set()\n",
    "    tokens_concepts: Set[str] = set()\n",
    "    \n",
    "    total_tokens_parcourus = 0\n",
    "    \n",
    "    # It√©ration sur les tokens (Cl√© = le mot/token)\n",
    "    for token, postings in base_index.items():\n",
    "        total_tokens_parcourus += 1\n",
    "        \n",
    "        token_nettoye = token.lower().strip()\n",
    "        if not token_nettoye:\n",
    "            continue\n",
    "            \n",
    "        est_plante = False\n",
    "        est_concept = False\n",
    "        \n",
    "        # It√©ration sur le dictionnaire de postings (Cl√© = doc_id, Valeur = fr√©quence)\n",
    "        for doc_id, frequence in postings.items(): \n",
    "            \n",
    "            # Classification bas√©e sur la fin du doc_id\n",
    "            if doc_id.endswith(PREFIXE_PLANTE):\n",
    "                est_plante = True\n",
    "            elif doc_id.endswith(PREFIXE_CONCEPT):\n",
    "                est_concept = True\n",
    "            \n",
    "            # Si nous avons trouv√© les deux cat√©gories, nous pouvons arr√™ter cette boucle interne\n",
    "            if est_plante and est_concept:\n",
    "                break \n",
    "                \n",
    "        # Stockage du token dans les ensembles finaux\n",
    "        if est_plante:\n",
    "            tokens_plantes.add(token_nettoye)\n",
    "        \n",
    "        if est_concept:\n",
    "            tokens_concepts.add(token_nettoye)\n",
    "            \n",
    "    print(f\"\\n--- R√©sultat du Tri ---\")\n",
    "    print(f\"Total des tokens uniques dans l'Index : {total_tokens_parcourus}\")\n",
    "    print(f\"Tokens class√©s comme 'Plantes' (JSON) : {len(tokens_plantes)}\")\n",
    "    print(f\"Tokens class√©s comme 'Concepts' (PDF) : {len(tokens_concepts)}\")\n",
    "    \n",
    "    return tokens_plantes, tokens_concepts\n",
    "\n",
    "# --- 4. Fonction de Sauvegarde ---\n",
    "\n",
    "def sauvegarder_vocabulaire(vocabulaire: Set[str], filepath: str, nom_type: str):\n",
    "    \"\"\"Sauvegarde l'ensemble de tokens tri√© dans un fichier JSON.\"\"\"\n",
    "    try:\n",
    "        liste_triee = sorted(list(vocabulaire))\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(liste_triee, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"‚úÖ Vocabulaire {nom_type} enregistr√© dans '{filepath}' ({len(liste_triee)} mots).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR lors de la sauvegarde du fichier {filepath} : {e}\")\n",
    "\n",
    "# --- 5. Ex√©cution du Script Principal ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"--- D√©marrage de la Reconstruction du Vocabulaire depuis l'Index ---\")\n",
    "\n",
    "    # 1. Chargement de la Base d'Index\n",
    "    base_index = charger_base_index(FICHIER_INDEX_SORTIE)\n",
    "    \n",
    "    if not base_index:\n",
    "        exit()\n",
    "\n",
    "    # 2. Extraction et Tri des Tokens\n",
    "    tokens_plantes, tokens_concepts = extraire_et_trier_tokens(base_index)\n",
    "    \n",
    "    # 3. Sauvegarde des Nouveaux Fichiers de Vocabulaire\n",
    "    sauvegarder_vocabulaire(tokens_plantes, FICHIER_TOKEN_PLANTE, \"Plantes (JSON)\")\n",
    "    sauvegarder_vocabulaire(tokens_concepts, FICHIER_TOKEN_CONCEPT, \"Concepts (PDF)\")\n",
    "\n",
    "    print(\"\\n--- Reconstruction du Vocabulaire bas√©e sur l'Index Termin√©e ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba0db41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chargement r√©ussi de 615 termes scientifiques/prot√©g√©s.\n",
      "‚úÖ Chargement r√©ussi de 3592 tokens de concepts.\n",
      "üîç D√©tection termin√©e. 78 concepts scientifiques trouv√©s.\n",
      "üíæ Succ√®s: Les concepts scientifiques ont √©t√© enregistr√©s dans science_concept.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# --- D√©finition des chemins de fichiers (Ajustez si n√©cessaire) ---\n",
    "# NOTE : J'utilise ici les noms de fichiers que vous avez fournis.\n",
    "TOKEN_CONCEPTS_FILE = 'token_concept.json'\n",
    "PROTECTED_TERMS_FILE = '../docs/mot_scientifique/protected_terms.json'\n",
    "OUTPUT_FILE = 'science_concept.json'\n",
    "\n",
    "\n",
    "def detect_scientific_concepts(tokens_path, protected_terms_path, output_path):\n",
    "    \"\"\"\n",
    "    D√©tecte les mots scientifiques/techniques dans la liste des tokens \n",
    "    en les comparant √† une liste de termes prot√©g√©s et g√©n√®re un fichier JSON.\n",
    "    \"\"\"\n",
    "    # 1. Chargement des termes prot√©g√©s (Mots Scientifiques)\n",
    "    try:\n",
    "        with open(protected_terms_path, 'r', encoding='utf-8') as f:\n",
    "            # protected_terms.json semble √™tre une simple liste de cha√Ænes\n",
    "            protected_terms_list = json.load(f)\n",
    "            # Convertir la liste en un ensemble (set) pour une recherche O(1) rapide\n",
    "            protected_terms_set = {term.lower() for term in protected_terms_list}\n",
    "        print(f\"‚úÖ Chargement r√©ussi de {len(protected_terms_set)} termes scientifiques/prot√©g√©s.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Erreur: Le fichier des termes prot√©g√©s n'a pas √©t√© trouv√© √† {protected_terms_path}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"‚ùå Erreur de d√©codage JSON dans le fichier {protected_terms_path}\")\n",
    "        return\n",
    "\n",
    "    # 2. Chargement des tokens de concepts\n",
    "    try:\n",
    "        with open(tokens_path, 'r', encoding='utf-8') as f:\n",
    "            # token_concept.json semble √™tre une simple liste de cha√Ænes\n",
    "            concept_tokens_list = json.load(f)\n",
    "        print(f\"‚úÖ Chargement r√©ussi de {len(concept_tokens_list)} tokens de concepts.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Erreur: Le fichier des tokens n'a pas √©t√© trouv√© √† {tokens_path}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"‚ùå Erreur de d√©codage JSON dans le fichier {tokens_path}\")\n",
    "        return\n",
    "\n",
    "    # 3. D√©tection des concepts scientifiques\n",
    "    scientific_concepts = set()\n",
    "    \n",
    "    # Parcourir chaque token et v√©rifier s'il est dans l'ensemble des termes prot√©g√©s.\n",
    "    # On met tout en minuscules pour assurer une correspondance insensible √† la casse.\n",
    "    for token in concept_tokens_list:\n",
    "        token_lower = token.lower()\n",
    "        if token_lower in protected_terms_set:\n",
    "            scientific_concepts.add(token) # Conserver la casse originale du token\n",
    "\n",
    "    scientific_concepts_list = sorted(list(scientific_concepts))\n",
    "    print(f\"üîç D√©tection termin√©e. {len(scientific_concepts_list)} concepts scientifiques trouv√©s.\")\n",
    "\n",
    "    # 4. Enregistrement du r√©sultat\n",
    "    try:\n",
    "        # Assurez-vous que le r√©pertoire de sortie existe\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir: # Ajout d'une v√©rification pour s'assurer que le chemin n'est pas vide\n",
    "             os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(scientific_concepts_list, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üíæ Succ√®s: Les concepts scientifiques ont √©t√© enregistr√©s dans {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l'enregistrement du fichier {output_path}: {e}\")\n",
    "\n",
    "# --- Ex√©cution du script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Ligne supprim√©e ou remplac√©e : ---\n",
    "    # L'ancien code causait l'erreur: os.makedirs(os.path.dirname(TOKEN_CONCEPTS_FILE), exist_ok=True)\n",
    "    # Remplac√© par une simple v√©rification de chemin si n√©cessaire :\n",
    "    if os.path.dirname(TOKEN_CONCEPTS_FILE):\n",
    "        os.makedirs(os.path.dirname(TOKEN_CONCEPTS_FILE), exist_ok=True)\n",
    "    \n",
    "    # Le script suppose que vous avez cr√©√© les fichiers d'entr√©e avant de l'ex√©cuter.\n",
    "    # Ex√©cutez la fonction de d√©tection\n",
    "    detect_scientific_concepts(TOKEN_CONCEPTS_FILE, PROTECTED_TERMS_FILE, OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
