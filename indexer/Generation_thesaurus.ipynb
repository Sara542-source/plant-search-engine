{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00537f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Downloading google_genai-1.54.0-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting anyio<5.0.0,>=4.8.0 (from google-genai)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-auth[requests]<3.0.0,>=2.14.1->google-genai)\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting httpx<1.0.0,>=0.28.1 (from google-genai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from google-genai)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai) (8.2.3)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.7)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.6.0)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.9.0->google-genai)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.11.0 (from google-genai)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.9.0->google-genai)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.2.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.4.8)\n",
      "Downloading google_genai-1.54.0-py3-none-any.whl (262 kB)\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m380.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: websockets, typing-extensions, rsa, typing-inspection, pydantic-core, google-auth, anyio, pydantic, httpx, google-genai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.20.1\n",
      "    Uninstalling pydantic_core-2.20.1:\n",
      "      Successfully uninstalled pydantic_core-2.20.1\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.2.0\n",
      "    Uninstalling anyio-4.2.0:\n",
      "      Successfully uninstalled anyio-4.2.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.8.2\n",
      "    Uninstalling pydantic-2.8.2:\n",
      "      Successfully uninstalled pydantic-2.8.2\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "Successfully installed anyio-4.12.0 google-auth-2.43.0 google-genai-1.54.0 httpx-0.28.1 pydantic-2.12.5 pydantic-core-2.41.5 rsa-4.9.1 typing-extensions-4.15.0 typing-inspection-0.4.2 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977dd020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cl√© API d√©finie dans l'environnement Python.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# üö® ATTENTION : Remplacez VOTRE_CL√â_API_ICI par votre cl√© r√©elle (ENTRE GUILLEMETS)\n",
    "# Ce code d√©finit la variable d'environnement pour cette session Python.\n",
    "\n",
    "os.environ['GEMINI_API_KEY'] = \"AIzaSyDcOXo5j3-3yzbjVo4FWEJYcxIW2Wp4uKM\" \n",
    "\n",
    "print(\"Cl√© API d√©finie dans l'environnement Python.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c352bff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client Gemini initialis√©.\n",
      "\n",
      "Mod√®les disponibles pour la g√©n√©ration de contenu :\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/gemini-2.0-flash-live-001\n",
      "models/gemini-live-2.5-flash-preview\n",
      "models/gemini-2.5-flash-live-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai.errors import APIError\n",
    "\n",
    "# --- Configuration de la cl√© API ---\n",
    "# Assurez-vous que la variable GEMINI_API_KEY est d√©finie dans votre environnement (os.environ)\n",
    "# ou utilisez os.environ['GEMINI_API_KEY'] = \"VOTRE_CL√â_ICI\" dans votre notebook.\n",
    "\n",
    "try:\n",
    "    # Le client lit automatiquement la cl√© GEMINI_API_KEY\n",
    "    client = genai.Client()\n",
    "    print(\"‚úÖ Client Gemini initialis√©.\")\n",
    "except Exception as e:\n",
    "    print(f\"üö® ERREUR : Impossible d'initialiser le client. D√©tail : {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Liste des Mod√®les ---\n",
    "print(\"\\nMod√®les disponibles pour la g√©n√©ration de contenu :\")\n",
    "try:\n",
    "    # La m√©thode 'models.list()' est utilis√©e pour obtenir la liste des mod√®les\n",
    "    models_list = client.models.list()\n",
    "    \n",
    "    # Filtrer et afficher les mod√®les\n",
    "    for model in models_list:\n",
    "        # Dans le nouveau SDK, la v√©rification est plus simple\n",
    "        if model.name.startswith(\"models/gemini\"):\n",
    "             print(model.name)\n",
    "\n",
    "except APIError as e:\n",
    "    print(f\"‚ùå ERREUR API lors de la liste des mod√®les : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a5f7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.generativeai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Mettez votre cl√© ici\u001b[39;00m\n\u001b[1;32m      4\u001b[0m genai\u001b[38;5;241m.\u001b[39mconfigure(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIzaSyDcOXo5j3-3yzbjVo4FWEJYcxIW2Wp4uKM\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.generativeai'"
     ]
    }
   ],
   "source": [
    "import \n",
    "\n",
    "# Mettez votre cl√© ici\n",
    "genai.configure(api_key=\"AIzaSyDcOXo5j3-3yzbjVo4FWEJYcxIW2Wp4uKM\" )\n",
    "\n",
    "print(\"Mod√®les disponibles :\")\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f9e52bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Initialisation r√©ussie (SpaCy & Gemini Client).\n",
      "--- 1. Extraction du Vocabulaire R√©el du Corpus ---\n",
      "‚úÖ Tokens extraits pour 'Arrosage' : 446 termes uniques.\n",
      "‚ö†Ô∏è AVERTISSEMENT : Fichier non trouv√© (../docs/concepts/pH du Sol .pdf.pdf).\n",
      "‚ö†Ô∏è AUCUN TEXTE VALIDE extrait pour 'pH du sol'.\n",
      "‚úÖ Tokens extraits pour 'Luminosit√©' : 556 termes uniques.\n",
      "\n",
      "[--- ENVOI DU PROMPT √Ä GEMINI (Attente ~30-45s) ---]\n",
      "‚ùå ERREUR API : √âchec de l'appel LLM. V√©rifiez votre cl√© et les quotas. D√©tail : 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 14.741708282s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '14s'}]}}\n",
      "\n",
      "--- Processus Termin√© ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Set\n",
    "import pypdf\n",
    "from google import genai\n",
    "from google.genai.errors import APIError\n",
    "\n",
    "# --- 1. CONFIGURATION GLOBALE ---\n",
    "\n",
    "# üö® METTEZ VOS CHEMINS R√âELS ICI\n",
    "DOSSIER_CONCEPTS_PDF = \"../docs/concepts\"\n",
    "FICHIER_STOCK_SCIENTIFIQUE = \"../docs/mot_scientifique/protected_terms.json\" \n",
    "FICHIER_THESAURUS_SORTIE = \"Thesaurus_test_concept.json\"\n",
    "\n",
    "LANGUE = 'fr_core_news_sm'\n",
    "\n",
    "# D√©finition des 3 concepts √† traiter et des fichiers associ√©s (A ADAPTER)\n",
    "CIBLES_CONCEPTS: Dict[str, List[str]] = {\n",
    "    \"Arrosage\": [os.path.join(DOSSIER_CONCEPTS_PDF, \"Arrosage.pdf\")],\n",
    "    \"pH du sol\": [os.path.join(DOSSIER_CONCEPTS_PDF, \"pH du Sol .pdf.pdf\")],\n",
    "    \"Luminosit√©\": [os.path.join(DOSSIER_CONCEPTS_PDF, \"Luminosit√© .pdf\")]\n",
    "}\n",
    "\n",
    "# --- 2. INITIALISATION ---\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(LANGUE)\n",
    "    client = genai.Client() # Lit la cl√© depuis GEMINI_API_KEY\n",
    "    print(\"‚úÖ Initialisation r√©ussie (SpaCy & Gemini Client).\")\n",
    "except OSError:\n",
    "    print(f\"üö® ERREUR : Mod√®le spaCy '{LANGUE}' non trouv√©. Ex√©cutez : python -m spacy download {LANGUE}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"üö® ERREUR d'initialisation : {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. CHARGEMENT DES RESSOURCES ---\n",
    "\n",
    "def charger_stock_scientifique(filepath: str) -> Set[str]:\n",
    "    \"\"\"Charge les termes scientifiques (mots prot√©g√©s) depuis un fichier JSON.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ö†Ô∏è AVERTISSEMENT : Fichier scientifique non trouv√© √† {filepath}. Retourne un ensemble vide.\")\n",
    "        return set()\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            # Normalisation en minuscules et remplacement des espaces par des underscores (si n√©cessaire)\n",
    "            stock = {item.lower().strip().replace(\" \", \"_\") for item in data if isinstance(item, str)}\n",
    "        return stock\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR lors du chargement du stock scientifique : {e}\")\n",
    "        return set()\n",
    "\n",
    "TERMES_PROTEGES: Set[str] = charger_stock_scientifique(FICHIER_STOCK_SCIENTIFIQUE)\n",
    "\n",
    "\n",
    "# --- 4. EXTRACTION ET TOKENISATION (Logique de l'Indexation) ---\n",
    "\n",
    "def extraire_texte_pdf(filepath: str) -> str:\n",
    "    \"\"\"Extrait le texte brut d'un PDF.\"\"\"\n",
    "    try:\n",
    "        texte = \"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            reader = pypdf.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                texte += page.extract_text() + \" \"\n",
    "        return texte\n",
    "    except pypdf.errors.PdfReadError:\n",
    "        print(f\"‚ùå ERREUR PDF : Impossible de lire le fichier {filepath} (corrompu ou prot√©g√©).\")\n",
    "        return \"\"\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERREUR FICHIER : Fichier non trouv√© √† {filepath}. V√©rifiez les chemins.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR d'extraction inattendue : {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def obtenir_tokens_pertinents(texte: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Applique le m√™me traitement NLP (Lemmatisation, Termes Prot√©g√©s) que votre indexeur.\n",
    "    Retourne la liste des tokens bruts pour le LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pr√©paration du texte : retire la ponctuation complexe, maintient les points d√©cimaux\n",
    "    texte_nettoye = re.sub(r'[^\\w\\s\\.\\,\\-\\']', ' ', texte).lower()\n",
    "\n",
    "    doc = nlp(texte_nettoye)\n",
    "    tokens_pertinents = []\n",
    "    \n",
    "    # 1. Lemmes et Filtration\n",
    "    for token in doc:\n",
    "        lemma_text = token.lemma_.strip()\n",
    "        # Conserver uniquement les lemmes alphab√©tiques significatifs (non stop words)\n",
    "        if (token.is_alpha and not token.is_stop and len(lemma_text) > 1):\n",
    "            tokens_pertinents.append(lemma_text)\n",
    "            \n",
    "    # 2. Ajout des Termes Prot√©g√©s (N-grammes, scientifiques)\n",
    "    # Les termes prot√©g√©s sont ajout√©s en tant que token unique (s√©par√© par '_')\n",
    "    texte_simple = texte.lower()\n",
    "    for terme in TERMES_PROTEGES:\n",
    "        if terme.replace(\"_\", \" \") in texte_simple:\n",
    "            tokens_pertinents.append(terme) # Ajoute le terme au format prot√©g√© (avec '_')\n",
    "\n",
    "    # 3. Ajout des valeurs num√©riques pertinentes (pH par exemple)\n",
    "    tokens_pertinents.extend(re.findall(r'(\\d[\\.,]\\d)', texte))\n",
    "    \n",
    "    # Retourner l'ensemble unique de tous les tokens (lemmes et termes prot√©g√©s)\n",
    "    return list(set(tokens_pertinents))\n",
    "\n",
    "\n",
    "# --- 5. G√âN√âRATION PAR LLM ---\n",
    "\n",
    "def generer_thesaurus_llm(concept_data: Dict[str, List[str]]) -> str:\n",
    "    \"\"\"\n",
    "    Construit le prompt unique avec les tokens et appelle l'API Gemini.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "Role: Tu es un expert en conception de th√©saurus (ISO 25964) pour la recherche botanique et agricole. Ton objectif est de g√©n√©rer trois entr√©es de th√©saurus compl√®tes au format JSON pour les Termes Pr√©f√©r√©s (TP) suivants : \"Arrosage\", \"pH du sol\" et \"Luminosit√©\".\n",
    "\n",
    "R√®gles de Contenu Stricte (Bas√©es sur le Vocabulaire Brut):\n",
    "1. S√âLECTION EXCLUSIVE : Les valeurs pour TP, NT, BT, et RT DOIVENT √äTRE CHOISIES UNIQUEMENT parmi les tokens exacts de la \"Liste de Vocabulaire Brut\" fournie ci-dessous pour chaque concept. Ces tokens sont les lemmes ou termes compos√©s (ex: stress_hydrique) de l'Index.\n",
    "2. ENRICHISSEMENT : La section UF (Synonymes/Non-Descripteurs) doit √™tre enrichie avec des variations linguistiques courantes (Darija, anglicismes) et des expressions usuelles que les utilisateurs pourraient taper, m√™me si elles ne sont pas dans la liste de vocabulaire brut.\n",
    "\n",
    "Format de Sortie Strict:\n",
    "G√©n√®re STRICTEMENT un seul objet JSON contenant les trois entr√©es. Utilise uniquement les cl√©s TP, UF, BT, NT, RT, et SN (Note d'Application).\n",
    "\n",
    "---\n",
    "### DONN√âES D'ENTR√âE BRUTES DU CORPUS\n",
    "---\n",
    "\"\"\"\n",
    "    # Ajout des donn√©es extraites du corpus au prompt\n",
    "    corpus_data_str = \"\"\n",
    "    for tp, vocab in concept_data.items():\n",
    "        # Utiliser json.dumps pour cr√©er une liste Python valide dans le prompt\n",
    "        safe_vocab = json.dumps(vocab, ensure_ascii=False)\n",
    "        corpus_data_str += f'\\nTP: \"{tp}\"\\nListe de Vocabulaire Brut: {safe_vocab}\\n'\n",
    "        \n",
    "    final_prompt = prompt_template + corpus_data_str + \"\\n--- SORTIE JSON ATTENDUE ---\\n\"\n",
    "    \n",
    "    print(\"\\n[--- ENVOI DU PROMPT √Ä GEMINI (Attente ~30-45s) ---]\")\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=final_prompt,\n",
    "            config={\"response_mime_type\": \"application/json\"} \n",
    "        )\n",
    "        return response.text\n",
    "        \n",
    "    except APIError as e:\n",
    "        print(f\"‚ùå ERREUR API : √âchec de l'appel LLM. V√©rifiez votre cl√© et les quotas. D√©tail : {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR inattendue lors de l'appel LLM : {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 6. SCRIPT PRINCIPAL (Ex√©cution) ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. EXTRACTION ET AGR√âGATION DU VOCABULAIRE\n",
    "    concept_vocabulaire: Dict[str, List[str]] = defaultdict(list)\n",
    "    \n",
    "    print(\"--- 1. Extraction du Vocabulaire R√©el du Corpus ---\")\n",
    "    total_files = sum(len(v) for v in CIBLES_CONCEPTS.values())\n",
    "    if total_files == 0:\n",
    "        print(\"‚ùå ERREUR : Aucun fichier configur√©. V√©rifiez CIBLES_CONCEPTS.\")\n",
    "        exit()\n",
    "\n",
    "    for concept, filepaths in CIBLES_CONCEPTS.items():\n",
    "        all_text = \"\"\n",
    "        for filepath in filepaths:\n",
    "            if os.path.exists(filepath):\n",
    "                all_text += extraire_texte_pdf(filepath) + \" \"\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è AVERTISSEMENT : Fichier non trouv√© ({filepath}).\")\n",
    "\n",
    "        if all_text.strip():\n",
    "            vocab = obtenir_tokens_pertinents(all_text)\n",
    "            concept_vocabulaire[concept] = vocab\n",
    "            print(f\"‚úÖ Tokens extraits pour '{concept}' : {len(vocab)} termes uniques.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è AUCUN TEXTE VALIDE extrait pour '{concept}'.\")\n",
    "\n",
    "    if not concept_vocabulaire or any(not v for v in concept_vocabulaire.values()):\n",
    "        print(\"\\n‚ùå Annulation : Le vocabulaire est incomplet ou vide. Le LLM ne sera pas appel√©.\")\n",
    "        exit()\n",
    "        \n",
    "    # 2. G√âN√âRATION DU THESAURUS PAR LLM\n",
    "    thesaurus_json_str = generer_thesaurus_llm(dict(concept_vocabulaire))\n",
    "    \n",
    "    # 3. SAUVEGARDE ET AFFICHAGE\n",
    "    if thesaurus_json_str:\n",
    "        try:\n",
    "            thesaurus_data = json.loads(thesaurus_json_str)\n",
    "            with open(FICHIER_THESAURUS_SORTIE, 'w', encoding='utf-8') as f:\n",
    "                json.dump(thesaurus_data, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "            print(f\"\\n‚úÖ Th√©saurus g√©n√©r√© avec succ√®s et enregistr√© dans '{FICHIER_THESAURUS_SORTIE}'.\")\n",
    "            print(\"\\n--- APER√áU DU R√âSULTAT FINAL ---\")\n",
    "            print(json.dumps(thesaurus_data, indent=2, ensure_ascii=False))\n",
    "            print(\"---------------------------------\")\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå ERREUR : Le LLM n'a pas renvoy√© un JSON valide. Sortie brute : {thesaurus_json_str[:500]}...\")\n",
    "        \n",
    "    print(\"\\n--- Processus Termin√© ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
